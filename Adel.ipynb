{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6UfwVwFaF4uY"
   },
   "outputs": [],
   "source": [
    "# Ячейка 1: Импорты и базовые настройки\n",
    "\n",
    "# Стандартная библиотека\n",
    "import json\n",
    "from math import radians\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "# Сторонние библиотеки\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import cdist\n",
    "import swifter\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    AutoConfig,\n",
    ")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.multiprocessing\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    classification_report,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    precision_recall_curve,\n",
    ")\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import joblib\n",
    "import shap\n",
    "from tqdm import tqdm\n",
    "from googletrans import Translator\n",
    "import googletrans\n",
    "\n",
    "# Локальные настройки\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "torch.multiprocessing.set_sharing_strategy(\"file_system\")\n",
    "\n",
    "print(googletrans.__version__)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Установка устройства\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Current device is: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WBm7vuKrF-fA"
   },
   "outputs": [],
   "source": [
    "# Ячейка 2: Пути к файлам и конфигурация\n",
    "main_dir = os.getenv(\"MAIN_DIR\")\n",
    "model_dir = os.path.join(main_dir, \"model_deberta_v3_xsmall\")\n",
    "data_dir = os.path.join(main_dir, \"liar2\")\n",
    "extra_data_dir = os.path.join(main_dir, \"liar-twitter\")\n",
    "unreliable_data_dir = os.path.join(main_dir, \"News_dataset\")\n",
    "scifact_dir = os.path.join(main_dir, \"SciFact\")\n",
    "fever_dir = os.path.join(main_dir, \"FEVER\")\n",
    "output_dir = os.path.join(main_dir, \"output\")\n",
    "intermediate_dir = os.path.join(main_dir, \"saved\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "class config:\n",
    "    MODEL = model_dir\n",
    "    MAX_LEN = 128\n",
    "    BATCH_SIZE_TRAIN = 16\n",
    "    BATCH_SIZE_VALID = 16\n",
    "    EPOCHS = 5\n",
    "    LEARNING_RATE = 2e-5\n",
    "    LEARNING_RATE_BIN = 4e-6\n",
    "    SEED = 42\n",
    "    NUM_CLASSES = 4  # меньше на 1, т.к. далее мы объединяем 0 и 1 классы\n",
    "    NUM_CLASSES_BIN = 1  # для бинарной модели\n",
    "    NUM_WORKERS = 4  # если мало памяти, лучше 0\n",
    "    GRADIENT_ACCUMULATION_STEPS = 4\n",
    "    WEIGHT_DECAY = 0.01\n",
    "    WEIGHT_DECAY_BIN = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zww_twgFGUM_"
   },
   "outputs": [],
   "source": [
    "# Ячейка 3: Функция для установки seed\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything(config.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ячейка 4.0: Подготовка к обработке данных\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "contractions = [\n",
    "    (r\"(\\w+)n['’]t\\b\", r\"\\1 not\"),  # couldn't -> could not\n",
    "    (r\"(\\w+)n t\\b\", r\"\\1 not\"),    # couldn t -> could not\n",
    "    (r\"(\\w+)['’]ll\\b\", r\"\\1 will\"),\n",
    "    (r\"(\\w+) ll\\b\", r\"\\1 will\"),\n",
    "    (r\"(\\w+)['’]re\\b\", r\"\\1 are\"),\n",
    "    (r\"(\\w+) re\\b\", r\"\\1 are\"),\n",
    "    (r\"(\\w+)['’]ve\\b\", r\"\\1 have\"),\n",
    "    (r\"(\\w+) ve\\b\", r\"\\1 have\"),\n",
    "    (r\"(\\w+)['’]m\\b\", r\"\\1 am\"),\n",
    "    (r\"(\\w+) m\\b\", r\"\\1 am\"),\n",
    "    (r\"(\\w+)['’]d\\b\", r\"\\1 would\"),\n",
    "    (r\"(\\w+) d\\b\", r\"\\1 would\"),\n",
    "    (r\"\\b(he|she|it|that|there|what|who|where)['’]s\\b\", r\"\\1 is\"),\n",
    "    (r\"\\b(he|she|it|that|there|what|who|where) s\\b\", r\"\\1 is\")\n",
    "]\n",
    "\n",
    "def clean_text(text):\n",
    "    for pattern, replacement in contractions:\n",
    "        text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)\n",
    "\n",
    "    text = re.sub(r'\\S+@\\S+', ' ', text)\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s\\.\\-\\+\\%\\$]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    cleaned_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.match(r'^[-+]?\\d*\\.?\\d+%?$', token):\n",
    "            cleaned_tokens.append(token)\n",
    "            continue\n",
    "        \n",
    "        lower_token = token.lower()\n",
    "        lemma = lemmatizer.lemmatize(lower_token)\n",
    "        \n",
    "        negation_words = {'not', 'no', 'nor', 'never', 'none', 'nothing', 'nowhere', 'n\\'t'}\n",
    "        if (lemma not in stop_words) or (lemma in negation_words):\n",
    "            cleaned_tokens.append(lemma)\n",
    "            \n",
    "    return ' '.join(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "8nNPvM4XG4j6"
   },
   "outputs": [],
   "source": [
    "# Ячейка 4.1: Загрузка данных liar2 (https://huggingface.co/datasets/chengxuphd/liar2)\n",
    "liar_train = pd.read_csv(os.path.join(data_dir, \"train.csv\"))\n",
    "liar_valid = pd.read_csv(os.path.join(data_dir, \"valid.csv\"))\n",
    "liar_test = pd.read_csv(os.path.join(data_dir, \"test.csv\"))\n",
    "\n",
    "liar_train = liar_train[['label', 'statement']]\n",
    "liar_valid = liar_valid[['label', 'statement']]\n",
    "liar_test = liar_test[['label', 'statement']]\n",
    "\n",
    "def liar_redo(df: pd.DataFrame):\n",
    "    # Объединяем 0 (pants-on-fire) и 1 (false) классы.\n",
    "    # Модель их плохо различает + они оба по сути ложные.\n",
    "    # Эти классы описывают лишь наглость лжи и без\n",
    "    # дополнительного контекста их будет сложно различить.\n",
    "    df['label'] = df['label'].replace({0:1})\n",
    "\n",
    "    # Теперь сдвигаем все классы на -1, чтобы классы начинались с 0\n",
    "    df['label'] = df['label'] - 1\n",
    "\n",
    "    df = df.rename(columns={'label': 'label_bin'})\n",
    "\n",
    "    # Оставляем только ложь и правду, промежуточные убираем\n",
    "    df['label_bin'] = df['label_bin'].apply(lambda x: 1 if x == 4 else 0 if x == 0 else 2)\n",
    "    df = df[df['label_bin'] != 2].reset_index(drop=True)\n",
    "\n",
    "    \n",
    "\n",
    "    return df\n",
    "\n",
    "liar_train = liar_redo(liar_train)\n",
    "liar_valid = liar_redo(liar_valid)\n",
    "liar_test = liar_redo(liar_test)\n",
    "\n",
    "liar_true_train = liar_train[liar_train['label_bin'] == 1]\n",
    "liar_false_train = liar_train[liar_train['label_bin'] == 0]\n",
    "\n",
    "liar_true_valid = liar_valid[liar_valid['label_bin'] == 1]\n",
    "liar_false_valid = liar_valid[liar_valid['label_bin'] == 0]\n",
    "\n",
    "liar_true_test = liar_test[liar_test['label_bin'] == 1]\n",
    "liar_false_test = liar_test[liar_test['label_bin'] == 0]\n",
    "\n",
    "print(f\"Train liar2: {liar_train['label_bin'].value_counts()}\")\n",
    "print(f\"Valid liar2: {liar_valid['label_bin'].value_counts()}\")\n",
    "print(f\"Test liar2: {liar_test['label_bin'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "f2LIad2J0aeg"
   },
   "outputs": [],
   "source": [
    "# Ячейка 4.2: Загрузка данных liar-twitter (https://www.kaggle.com/datasets/muhammadimran112233/liar-twitter-dataset/data)\n",
    "liar_twitter = pd.read_csv(os.path.join(extra_data_dir, \"Liar_Dataset.csv\"))\n",
    "\n",
    "label_mapping = {\"pants-fire\": 0, \"FALSE\": 1, \"barely-true\": 2, \"half-true\": 3, \"mostly-true\": 4, \"TRUE\": 5}\n",
    "liar_twitter['label'] = liar_twitter['label'].map(label_mapping)\n",
    "\n",
    "liar_twitter = liar_twitter[['label', 'statement']]\n",
    "liar_twitter = liar_redo(liar_twitter)\n",
    "\n",
    "liar_twitter_true = liar_twitter[liar_twitter['label_bin'] == 1].reset_index(drop=True)\n",
    "liar_twitter_false = liar_twitter[liar_twitter['label_bin'] == 0].reset_index(drop=True)\n",
    "\n",
    "def split_train_valid_test(df: pd.DataFrame):\n",
    "    df_train, df_valid_test = train_test_split(\n",
    "        df,\n",
    "        test_size=0.2,\n",
    "        random_state=config.SEED\n",
    "    )\n",
    "    df_valid, df_test = train_test_split(\n",
    "        df_valid_test,\n",
    "        test_size=0.5,\n",
    "        random_state=config.SEED\n",
    "    )\n",
    "    return df_train, df_valid, df_test\n",
    "\n",
    "liar_twitter_true_train, liar_twitter_true_valid, liar_twitter_true_test = split_train_valid_test(liar_twitter_true)\n",
    "liar_twitter_false_train, liar_twitter_false_valid, liar_twitter_false_test = split_train_valid_test(liar_twitter_false)\n",
    "\n",
    "liar_twitter_train = pd.concat([liar_twitter_true_train, liar_twitter_false_train]).reset_index(drop=True)\n",
    "liar_twitter_valid = pd.concat([liar_twitter_true_valid, liar_twitter_false_valid]).reset_index(drop=True)\n",
    "liar_twitter_test  = pd.concat([liar_twitter_true_test, liar_twitter_false_test]).reset_index(drop=True)\n",
    "\n",
    "print(f\"Train liar-twitter: {liar_twitter_train['label_bin'].value_counts()}\")\n",
    "print(f\"Valid liar-twitter: {liar_twitter_valid['label_bin'].value_counts()}\")\n",
    "print(f\"Test liar-twitter: {liar_twitter_test['label_bin'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "02qUW0-KIh4G"
   },
   "outputs": [],
   "source": [
    "# Ячейка 4.3: Загрузка данных Fake News (https://www.kaggle.com/datasets/emineyetm/fake-news-detection-datasets/data)\n",
    "\n",
    "# unreliable_false = pd.read_csv(os.path.join(unreliable_data_dir, \"Fake.csv\"))\n",
    "# unreliable_true = pd.read_csv(os.path.join(unreliable_data_dir, \"True.csv\"))\n",
    "\n",
    "# unreliable_false['statement'] = unreliable_false['text']\n",
    "# unreliable_true['statement']  = unreliable_true['text']\n",
    "\n",
    "# unreliable_false = unreliable_false[['statement']]\n",
    "# unreliable_true = unreliable_true[['statement']]\n",
    "\n",
    "# unreliable_false['label_bin'] = 0\n",
    "# unreliable_true['label_bin'] = 1\n",
    "\n",
    "# def clean_false(text_series):\n",
    "#     return text_series.str.replace(r'((\\s?[a-zA-Z,.-]+){1,4}\\s+?\\(@.+?\\)\\s[a-zA-Z]+\\s[0-3]?[0-9],?\\s?20[0-9][0-9].+$)|(((https://)|(([a-zA-Z]+?\\.){1,3}[a-zA-Z]+?/))[a-zA-Z./0-9]+\\s)|(((Featured image via)|(Photo by)|(Image via)).+$)', '', regex=True)\n",
    "\n",
    "# def clean_truths(text_series):\n",
    "#     return text_series.str.replace(r'(^.*?-\\s*)|(\\s*-\\sSource link.*$)|(((bit\\.ly.+?)|(via\\s+@[a-zA-Z0-9]+))?\\s*\\[.+?\\]\\s*-)', '', regex=True)\n",
    "\n",
    "# def parallelize_series(series, func, n_cores=None):\n",
    "#     if n_cores is None:\n",
    "#         n_cores = cpu_count()\n",
    "#     chunk_size = int(np.ceil(len(series) / n_cores))\n",
    "#     chunks = [series[i*chunk_size:(i+1)*chunk_size] for i in range(n_cores)]\n",
    "\n",
    "#     with Pool(n_cores) as pool:\n",
    "#         results = pool.map(func, chunks)\n",
    "#     return pd.concat(results)\n",
    "\n",
    "\n",
    "# for text in unreliable_false['statement'][0:1]:\n",
    "#   print(text)\n",
    "\n",
    "# unreliable_false['statement'] = parallelize_series(unreliable_false['statement'], clean_false, 10)\n",
    "# unreliable_false['statement'] = unreliable_false['statement'].apply(lambda x: clean_text(x))\n",
    "# unreliable_false['statement'] = unreliable_false['statement'].fillna(\"\").astype(str)\n",
    "# unreliable_false = unreliable_false[unreliable_false['statement'].str.strip() != \"\"].reset_index(drop=True)\n",
    "\n",
    "# for text in unreliable_false['statement'][0:1]:\n",
    "#   print(text)\n",
    "\n",
    "# for text in unreliable_true['statement'][0:1]:\n",
    "#   print(text)\n",
    "\n",
    "# unreliable_true['statement'] = parallelize_series(unreliable_true['statement'], clean_truths, 10)\n",
    "# unreliable_true['statement'] = unreliable_true['statement'].apply(lambda x: clean_text(x))\n",
    "# unreliable_true['statement'] = unreliable_true['statement'].fillna(\"\").astype(str)\n",
    "# unreliable_true = unreliable_true[unreliable_true['statement'].str.strip() != \"\"].reset_index(drop=True)\n",
    "\n",
    "# for text in unreliable_true['statement'][0:1]:\n",
    "#   print(text)\n",
    "\n",
    "# # Сохраняем обработанные датафреймы в CSV\n",
    "# unreliable_false.to_csv(os.path.join(unreliable_data_dir, \"unreliable_false_cleaned.csv\"), index=False)\n",
    "# unreliable_true.to_csv(os.path.join(unreliable_data_dir, \"unreliable_true_cleaned.csv\"), index=False)\n",
    "\n",
    "# Читаем обработанные датафреймы из CSV\n",
    "unreliable_true = pd.read_csv(os.path.join(unreliable_data_dir, \"unreliable_true_cleaned.csv\"))\n",
    "unreliable_false = pd.read_csv(os.path.join(unreliable_data_dir, \"unreliable_false_cleaned.csv\"))\n",
    "\n",
    "unreliable_true_train, unreliable_true_valid, unreliable_true_test = split_train_valid_test(unreliable_true)\n",
    "unreliable_false_train,unreliable_false_valid, unreliable_false_test = split_train_valid_test(unreliable_false)\n",
    "\n",
    "unreliable_train = pd.concat([unreliable_true_train, unreliable_false_train]).reset_index(drop=True)\n",
    "unreliable_valid = pd.concat([unreliable_true_valid, unreliable_false_valid]).reset_index(drop=True)\n",
    "unreliable_test  = pd.concat([unreliable_true_test, unreliable_false_test]).reset_index(drop=True)\n",
    "\n",
    "print(f\"Train fake-news: {unreliable_train['label_bin'].value_counts()}\")\n",
    "print(f\"Valid fake-news: {unreliable_valid['label_bin'].value_counts()}\")\n",
    "print(f\"Test fake-news: {unreliable_test['label_bin'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ячейка 4.4: Загрузка данных SCiFact (https://www.kaggle.com/datasets/thedevastator/unlock-insight-into-scientific-claims-with-scifa)\n",
    "\n",
    "scifact_train = pd.read_csv(os.path.join(scifact_dir, \"claims_train.csv\"))\n",
    "scifact_valid = pd.read_csv(os.path.join(scifact_dir, \"claims_validation.csv\"))\n",
    "\n",
    "scifact_all = pd.concat([scifact_train, scifact_valid])\n",
    "\n",
    "scifact_all = scifact_all.rename(columns={'claim': 'statement'})\n",
    "scifact_all = scifact_all.rename(columns={'evidence_label': 'label_bin'})\n",
    "scifact_all = scifact_all.drop(columns=['id', 'evidence_doc_id', 'evidence_sentences', 'cited_doc_ids'])\n",
    "scifact_all = scifact_all.dropna()\n",
    "scifact_all['label_bin'] = scifact_all['label_bin'].apply(lambda x: 1 if x == \"SUPPORT\" else 0)\n",
    "\n",
    "scifact_true = scifact_all[scifact_all['label_bin'] == 1].reset_index(drop=True)\n",
    "scifact_false = scifact_all[scifact_all['label_bin'] == 0].reset_index(drop=True)\n",
    "\n",
    "scifact_true_train, scifact_true_valid, scifact_true_test = split_train_valid_test(scifact_true)\n",
    "scifact_false_train, scifact_false_valid, scifact_false_test = split_train_valid_test(scifact_false)\n",
    "\n",
    "scifact_train = pd.concat([scifact_true_train, scifact_false_train]).reset_index(drop=True)\n",
    "scifact_valid = pd.concat([scifact_true_valid, scifact_false_valid]).reset_index(drop=True)\n",
    "scifact_test  = pd.concat([scifact_true_test, scifact_false_test]).reset_index(drop=True)\n",
    "\n",
    "print(f\"scifact_train:\\n{scifact_train['label_bin'].value_counts()}\")\n",
    "print(f\"scifact_valid:\\n{scifact_valid['label_bin'].value_counts()}\")\n",
    "print(f\"scifact_test:\\n{scifact_test['label_bin'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ячейка 4.5: Загрузка данных FEVER (https://fever.ai/dataset/fever.html)\n",
    "\n",
    "fever = pd.read_json(os.path.join(fever_dir, \"train.jsonl\"), lines=True)\n",
    "\n",
    "fever = fever[fever['verifiable'] == \"VERIFIABLE\"].drop(columns=['verifiable', 'id', 'evidence'])\n",
    "fever = fever[fever['label'] != \"NOT ENOUGH INFO\"].rename(columns={'label': 'label_bin', 'claim': 'statement'})\n",
    "fever = fever.dropna().reset_index(drop=True)\n",
    "fever['label_bin'] = fever['label_bin'].apply(lambda x: 1 if x == \"SUPPORTS\" else 0)\n",
    "\n",
    "fever_true = fever[fever['label_bin'] == 1]\n",
    "fever_false = fever[fever['label_bin'] == 0]\n",
    "\n",
    "fever_true_train, fever_true_valid, fever_true_test = split_train_valid_test(fever_true)\n",
    "fever_false_train, fever_false_valid, fever_false_test = split_train_valid_test(fever_false)\n",
    "\n",
    "fever_train = pd.concat([fever_true_train, fever_false_train]).reset_index(drop=True)\n",
    "fever_valid = pd.concat([fever_true_valid, fever_false_valid]).reset_index(drop=True)\n",
    "fever_test  = pd.concat([fever_true_test, fever_false_test]).reset_index(drop=True)\n",
    "\n",
    "print(f\"fever_train:\\n{fever_train['label_bin'].value_counts()}\")\n",
    "print(f\"fever_valid:\\n{fever_valid['label_bin'].value_counts()}\")\n",
    "print(f\"fever_test:\\n{fever_test['label_bin'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5qUyniPeKFYs"
   },
   "outputs": [],
   "source": [
    "# Ячейка 5: Объединение.\n",
    "datasets = ['liar', 'liar_twitter', 'unreliable', 'scifact', 'fever']\n",
    "splits = ['train', 'valid', 'test']\n",
    "\n",
    "for ds in datasets:\n",
    "    for split in splits:\n",
    "        df = globals()[f\"{ds}_{split}\"]\n",
    "        df['dataset_name'] = ds\n",
    "\n",
    "# Объединяем датасеты\n",
    "train_df = pd.concat([globals()[f\"{ds}_train\"] for ds in datasets]).reset_index(drop=True)\n",
    "valid_df = pd.concat([globals()[f\"{ds}_valid\"] for ds in datasets]).reset_index(drop=True)\n",
    "test_df = pd.concat([globals()[f\"{ds}_test\"] for ds in datasets]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ячейка 6: Форматируем данные в датасетах.\n",
    "train_df['statement'] = train_df['statement'].swifter.apply(clean_text)\n",
    "valid_df['statement'] = valid_df['statement'].swifter.apply(clean_text)\n",
    "test_df['statement']  = test_df['statement'].swifter.apply(clean_text)\n",
    "\n",
    "train_df = train_df.drop_duplicates(subset=['statement'], keep='first').reset_index(drop=True)\n",
    "valid_df = valid_df.drop_duplicates(subset=['statement'], keep='first').reset_index(drop=True)\n",
    "test_df  = test_df.drop_duplicates(subset=['statement'], keep='first').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ячейка 7.0: Подготовка к редукции датасета.\n",
    "def compute_embeddings_and_labels(texts, \n",
    "                                  num_clusters=500, \n",
    "                                  svd_components=50, \n",
    "                                  tsne_perplexity=40, \n",
    "                                  tsne_iter=300, \n",
    "                                  random_seed=config.SEED,\n",
    "                                  verbose=True):\n",
    "    \"\"\"\n",
    "    Векторизация, понижение размерности, кластеризация и t-SNE.\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1, 2))\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    \n",
    "    svd = TruncatedSVD(n_components=svd_components, random_state=random_seed)\n",
    "    X_reduced = svd.fit_transform(X)\n",
    "    \n",
    "    kmeans = MiniBatchKMeans(n_clusters=num_clusters, random_state=random_seed)\n",
    "    kmeans.fit(X_reduced)\n",
    "    labels = kmeans.labels_\n",
    "    \n",
    "    tsne = TSNE(n_components=2, perplexity=tsne_perplexity, n_iter=tsne_iter, random_state=random_seed)\n",
    "    X_embedded = tsne.fit_transform(X_reduced)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Вычислено эмбеддингов: {len(texts)}\")\n",
    "\n",
    "    # Визуализация t-SNE\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    scatter = ax.scatter(X_embedded[:, 0], X_embedded[:, 1], c=labels, cmap='tab20', s=5, alpha=0.7)\n",
    "    plt.colorbar(scatter, ticks=range(len(np.unique(labels))))\n",
    "    ax.set_title(\"Кластеризация текстов (без эллипсов)\")\n",
    "    plt.xlabel(\"t-SNE компонента 1\")\n",
    "    plt.ylabel(\"t-SNE компонента 2\")\n",
    "    plt.show()\n",
    "    \n",
    "    return X_reduced, X_embedded, labels, kmeans\n",
    "\n",
    "def get_top_n_cluster_examples(texts, X_reduced, kmeans, n=5):\n",
    "    centers = kmeans.cluster_centers_\n",
    "    distances = cdist(centers, X_reduced, metric='euclidean')\n",
    "    \n",
    "    top_n_indices_per_cluster = np.argsort(distances, axis=1)[:, :n]\n",
    "    \n",
    "    cluster_examples = []\n",
    "    cluster_example_indices = []\n",
    "    for i in range(len(centers)):\n",
    "        indices = top_n_indices_per_cluster[i]\n",
    "        examples = [texts[idx] for idx in indices]\n",
    "        cluster_examples.append(examples)\n",
    "        cluster_example_indices.append(indices)\n",
    "    \n",
    "    return cluster_examples, cluster_example_indices\n",
    "\n",
    "def points_in_ellipse(X, center, axes, angle_rad):\n",
    "    cos_angle = np.cos(angle_rad)\n",
    "    sin_angle = np.sin(angle_rad)\n",
    "    xc, yc = center\n",
    "    a, b = axes\n",
    "    diff = X - np.array([xc, yc])\n",
    "    x_rot = diff[:, 0]*cos_angle + diff[:, 1]*sin_angle\n",
    "    y_rot = -diff[:, 0]*sin_angle + diff[:, 1]*cos_angle\n",
    "    val = (x_rot/a)**2 + (y_rot/b)**2\n",
    "    return val <= 1\n",
    "\n",
    "def plot_ellipses(X_embedded, labels, ellipses_params, show=True):\n",
    "    \"\"\"\n",
    "    Рисует scatter и несколько эллипсов.\n",
    "    Возвращает список массивов индексов точек внутри каждого эллипса.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12,8))\n",
    "    scatter = ax.scatter(X_embedded[:,0], X_embedded[:,1], c=labels, cmap='tab20', s=5, alpha=0.7)\n",
    "    \n",
    "    indices_list = []\n",
    "    for params in ellipses_params:\n",
    "        ellipse = Ellipse(\n",
    "            xy=params['center'],\n",
    "            width=params['width'],\n",
    "            height=params['height'],\n",
    "            angle=np.degrees(params['angle_rad']),\n",
    "            edgecolor='red', facecolor='none', linewidth=2\n",
    "        )\n",
    "        ax.add_patch(ellipse)\n",
    "\n",
    "        axes = (params['width']/2, params['height']/2)\n",
    "        mask = points_in_ellipse(X_embedded, params['center'], axes, params['angle_rad'])\n",
    "        indices = np.where(mask)[0]\n",
    "        indices_list.append(indices)\n",
    "\n",
    "        print(f\"Эллипс в {params['center']}: {len(indices)} точек найдено\")\n",
    "    \n",
    "    if show:\n",
    "        plt.colorbar(scatter, ticks=range(len(np.unique(labels))))\n",
    "        ax.set_title(\"Кластеризация с эллипсами\")\n",
    "        plt.xlabel(\"Компонента 1\")\n",
    "        plt.ylabel(\"Компонента 2\")\n",
    "        plt.show()\n",
    "    \n",
    "    return indices_list\n",
    "\n",
    "def reduce_texts_by_indices(texts, labels, indices_list, keep_fraction=0.2, random_seed=config.SEED):\n",
    "    \"\"\"\n",
    "    По списку индексов выполняет редуцирование:\n",
    "    из кластеров, плотно попадающих в эллипс, оставляет keep_fraction примеров случайно,\n",
    "    возвращает новый общий список текстов с уменьшенным размером.\n",
    "    \"\"\"\n",
    "    all_filtered_indices = []\n",
    "    all_reduced_texts = []\n",
    "\n",
    "    labels_df = pd.Series(labels).value_counts()\n",
    "\n",
    "    for indices_in_ellipse in indices_list:\n",
    "        labels_in_ellipse = labels[indices_in_ellipse]\n",
    "        labels_ellipse_df = pd.Series(labels_in_ellipse).value_counts()\n",
    "\n",
    "        to_reduce = []\n",
    "        for label_id in labels_ellipse_df.index:\n",
    "            if labels_df[label_id] * 0.8 <= labels_ellipse_df[label_id]:\n",
    "                to_reduce.append(label_id)\n",
    "\n",
    "        # Фильтруем индексы для кластеров, которые нужно редуцировать\n",
    "        mask = np.isin(labels_in_ellipse, to_reduce)\n",
    "        filtered_indices = indices_in_ellipse[mask]\n",
    "\n",
    "        df_sel = pd.DataFrame({\n",
    "            'index': filtered_indices,\n",
    "            'label': labels[filtered_indices],\n",
    "            'text': [texts[i] for i in filtered_indices]\n",
    "        })\n",
    "\n",
    "        def sample_fn(group):\n",
    "            n_keep = max(1, int(len(group)*keep_fraction))\n",
    "            return group.sample(n=n_keep, random_state=random_seed)\n",
    "\n",
    "        reduced_df = df_sel.groupby('label').apply(sample_fn).reset_index(drop=True)\n",
    "\n",
    "        all_filtered_indices.extend(list(filtered_indices))\n",
    "        all_reduced_texts.extend(reduced_df['text'].tolist())\n",
    "\n",
    "        print(f\"Область эллипса: {len(indices_in_ellipse)} исходных, редуцировано до {len(reduced_df)}\")\n",
    "\n",
    "    # Формируем итоговый список: вынести удалённые и добавить редуцированные\n",
    "    set_to_remove = set(all_filtered_indices)\n",
    "    filtered_texts = [texts[i] for i in range(len(texts)) if i not in set_to_remove]\n",
    "    combined_texts = filtered_texts + all_reduced_texts\n",
    "\n",
    "    print(f\"Общий размер после редукции: {len(combined_texts)}\")\n",
    "    return combined_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ячейка 7.1: Получение текстов для кластеризации.\n",
    "texts = train_df['statement'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ячейка 7.2: Флаг, отвечающий за вычисление кластеров.\n",
    "# Если изменений в данных не производилось, то лучше не трогать, т.к. это дорогостоящая операция.\n",
    "run_flag = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ячейка 7.3: Первая кластеризация\n",
    "if not run_flag:\n",
    "    # Загрузка\n",
    "    X_reduced = np.load(os.path.join(intermediate_dir, 'X_reduced.npy'))\n",
    "    X_embedded = np.load(os.path.join(intermediate_dir, 'X_embedded.npy'))\n",
    "    labels = np.load(os.path.join(intermediate_dir, 'labels.npy'))\n",
    "    kmeans = joblib.load(os.path.join(intermediate_dir, 'kmeans_model.joblib'))\n",
    "    print(f\"Загружены данные из {intermediate_dir}\")\n",
    "    # plot_ellipses(X_embedded, labels, ellipses_params=[])\n",
    "else:\n",
    "    X_reduced, X_embedded, labels, kmeans = compute_embeddings_and_labels(texts)\n",
    "\n",
    "    # Сохранение\n",
    "    np.save(os.path.join(intermediate_dir, 'X_reduced.npy'), X_reduced)\n",
    "    np.save(os.path.join(intermediate_dir, 'X_embedded.npy'), X_embedded)\n",
    "    np.save(os.path.join(intermediate_dir, 'labels.npy'), labels)\n",
    "    joblib.dump(kmeans, os.path.join(intermediate_dir, 'kmeans_model.joblib'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ячейка 7.4: Первый выбор областей для редукции.\n",
    "ellipse_params = [{\n",
    "    'center': (1, 10.5),\n",
    "    'width': 4.5,\n",
    "    'height': 8,\n",
    "    'angle_rad': radians(-10)\n",
    "}, {\n",
    "    'center': (8.5, 2),\n",
    "    'width': 14,\n",
    "    'height': 16,\n",
    "    'angle_rad': radians(0)\n",
    "}, {\n",
    "    'center': (-0.75, -4.75),\n",
    "    'width': 6,\n",
    "    'height': 7,\n",
    "    'angle_rad': radians(0)\n",
    "}]\n",
    "\n",
    "indices_list = plot_ellipses(X_embedded, labels, ellipse_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ячейка 7.5: Первое выполнение редукции.\n",
    "reduced_texts = reduce_texts_by_indices(texts, labels, indices_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not run_flag:\n",
    "    # Загрузка\n",
    "    X_reduced2 = np.load(os.path.join(intermediate_dir, 'X_reduced2.npy'))\n",
    "    X_embedded2 = np.load(os.path.join(intermediate_dir, 'X_embedded2.npy'))\n",
    "    labels2 = np.load(os.path.join(intermediate_dir, 'labels2.npy'))\n",
    "    kmeans2 = joblib.load(os.path.join(intermediate_dir, 'kmeans2_model.joblib'))\n",
    "    print(f\"Загружены данные из {intermediate_dir}\")\n",
    "    # plot_ellipses(X_embedded2, labels2, ellipses_params=[])\n",
    "else:\n",
    "    X_reduced2, X_embedded2, labels2, kmeans2 = compute_embeddings_and_labels(reduced_texts)\n",
    "\n",
    "    # Сохранение\n",
    "    np.save(os.path.join(intermediate_dir, 'X_reduced2.npy'), X_reduced2)\n",
    "    np.save(os.path.join(intermediate_dir, 'X_embedded2.npy'), X_embedded2)\n",
    "    np.save(os.path.join(intermediate_dir, 'labels2.npy'), labels2)\n",
    "    joblib.dump(kmeans2, os.path.join(intermediate_dir, 'kmeans2_model.joblib'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ellipse_params2 = [{\n",
    "    'center': (2.5, -6),\n",
    "    'width': 6,\n",
    "    'height': 7,\n",
    "    'angle_rad': radians(0)\n",
    "}, {\n",
    "    'center': (-5.75, -2.25),\n",
    "    'width': 12,\n",
    "    'height': 7,\n",
    "    'angle_rad': radians(-35)\n",
    "}, {\n",
    "    'center': (-5, -12),\n",
    "    'width': 7,\n",
    "    'height': 5,\n",
    "    'angle_rad': radians(45)\n",
    "}]\n",
    "\n",
    "indices_list2 = plot_ellipses(X_embedded2, labels2, ellipse_params2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_texts2 = reduce_texts_by_indices(reduced_texts, labels2, indices_list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not run_flag:\n",
    "    # Загрузка\n",
    "    X_reduced3 = np.load(os.path.join(intermediate_dir, 'X_reduced3.npy'))\n",
    "    X_embedded3 = np.load(os.path.join(intermediate_dir, 'X_embedded3.npy'))\n",
    "    labels3 = np.load(os.path.join(intermediate_dir, 'labels3.npy'))\n",
    "    kmeans3 = joblib.load(os.path.join(intermediate_dir, 'kmeans3_model.joblib'))\n",
    "    print(f\"Загружены данные из {intermediate_dir}\")\n",
    "    # plot_ellipses(X_embedded3, labels3, ellipses_params=[])\n",
    "else:\n",
    "    X_reduced3, X_embedded3, labels3, kmeans3 = compute_embeddings_and_labels(reduced_texts2)\n",
    "    # Сохранение\n",
    "    np.save(os.path.join(intermediate_dir, 'X_reduced3.npy'), X_reduced3)\n",
    "    np.save(os.path.join(intermediate_dir, 'X_embedded3.npy'), X_embedded3)\n",
    "    np.save(os.path.join(intermediate_dir, 'labels3.npy'), labels3)\n",
    "    joblib.dump(kmeans3, os.path.join(intermediate_dir, 'kmeans3_model.joblib'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ячейка 8.1: Выбор строк из оригинального тренировочного массива по редуцированному набору.\n",
    "reduced_set = set(reduced_texts2)\n",
    "reduced_train_df = train_df[pd.Index(texts).isin(reduced_set)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ячейка 8.2: Вычисление выбросов среди редуцированного набора и их удаление.\n",
    "\n",
    "# Создаём DataFrame признаков из эмбеддингов\n",
    "df_features = pd.DataFrame(X_reduced3)\n",
    "\n",
    "# Добавляем колонку с именем датасета из согласованного reduced_train_df\n",
    "df_features['dataset_name'] = reduced_train_df['dataset_name'].values\n",
    "\n",
    "# Применяем Isolation Forest по группам\n",
    "\n",
    "def detect_outliers_by_group(df_feat, group_col='dataset_name', contamination=0.05):\n",
    "    outliers_idx = []\n",
    "    for group in df_feat[group_col].unique():\n",
    "        grp_data = df_feat[df_feat[group_col] == group].drop(columns=[group_col])\n",
    "        iso_forest = IsolationForest(contamination=contamination, random_state=config.SEED)\n",
    "        preds = iso_forest.fit_predict(grp_data)\n",
    "        group_outliers = df_feat[df_feat[group_col] == group].index[preds == -1].tolist()\n",
    "        outliers_idx.extend(group_outliers)\n",
    "        print(f\"Группа '{group}': найдено выбросов {len(group_outliers)}\")\n",
    "    return outliers_idx\n",
    "\n",
    "outliers_indices = detect_outliers_by_group(df_features)\n",
    "\n",
    "# Добавляем метку выбросов в reduced_train_df\n",
    "reduced_train_df['is_outlier'] = False\n",
    "reduced_train_df.loc[outliers_indices, 'is_outlier'] = True\n",
    "\n",
    "print(f\"Найдено выбросов в reduced_train_df: {reduced_train_df['is_outlier'].sum()} из {len(reduced_train_df)}\")\n",
    "\n",
    "reduced_train_df = reduced_train_df[reduced_train_df['is_outlier'] == False].reset_index(drop=True)\n",
    "reduced_texts3 = reduced_train_df['statement'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ячейка 8.3: Визуализация сокращения данных в числовом выражении.\n",
    "for ds in datasets:\n",
    "    filter_df = train_df[train_df['dataset_name'] == ds]\n",
    "    true_df = filter_df[filter_df['label_bin'] == 1]\n",
    "    reduced_filter_df = reduced_train_df[reduced_train_df['dataset_name'] == ds]\n",
    "    reduced_true_df = reduced_filter_df[reduced_filter_df['label_bin'] == 1]\n",
    "    \n",
    "    print(f\"{ds}:\\nTrue: {len(true_df)} -> {len(reduced_true_df)}\\nFalse: {len(filter_df) - len(true_df)} -> {len(reduced_filter_df) - len(reduced_true_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ячейка 9.0: Выполняем кластеризацию для выделения центральных примеров из кластеров.\n",
    "if not run_flag:\n",
    "    # Загрузка\n",
    "    X_reduced4 = np.load(os.path.join(intermediate_dir, 'X_reduced4.npy'))\n",
    "    X_embedded4 = np.load(os.path.join(intermediate_dir, 'X_embedded4.npy'))\n",
    "    labels4 = np.load(os.path.join(intermediate_dir, 'labels4.npy'))\n",
    "    kmeans4 = joblib.load(os.path.join(intermediate_dir, 'kmeans4_model.joblib'))\n",
    "    print(f\"Загружены данные из {intermediate_dir}\")\n",
    "    # plot_ellipses(X_embedded4, labels4, ellipses_params=[])\n",
    "else:\n",
    "    X_reduced4, X_embedded4, labels4, kmeans4 = compute_embeddings_and_labels(reduced_texts3)\n",
    "    # Сохранение\n",
    "    np.save(os.path.join(intermediate_dir, 'X_reduced4.npy'), X_reduced4)\n",
    "    np.save(os.path.join(intermediate_dir, 'X_embedded4.npy'), X_embedded4)\n",
    "    np.save(os.path.join(intermediate_dir, 'labels4.npy'), labels4)\n",
    "    joblib.dump(kmeans4, os.path.join(intermediate_dir, 'kmeans4_model.joblib'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ячейка 9.1: Выделение центральных примеров из кластеров.\n",
    "center_texts, center_indices = get_top_n_cluster_examples(reduced_texts3, X_reduced4, kmeans4, n=20)\n",
    "for i, text in enumerate(center_texts[:10]):\n",
    "    print(f\"Кластер {i} ключевое предложение:\\n{text}\\n\")\n",
    "\n",
    "all_center_texts = []\n",
    "for text_list in center_texts:\n",
    "    all_center_texts.extend(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ячейка 9.2: Формирование датафрейма из центральных примеров.\n",
    "center_texts_set = set(all_center_texts)\n",
    "core_train_df = reduced_train_df[reduced_train_df['statement'].isin(center_texts_set)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ячейка 10.0: Функция расчёта весов классов каждого датасета по отношению к размеру общего.\n",
    "def compute_sample_weights(df, label_col='label_bin', dataset_col='dataset_name'):\n",
    "    total_len = len(df)\n",
    "    weights_true = df[df[label_col] == 1][dataset_col].value_counts()\n",
    "    weights_false = df[df[label_col] == 0][dataset_col].value_counts()\n",
    "\n",
    "    w_true = total_len / weights_true\n",
    "    w_false = total_len / weights_false\n",
    "    total_weights_sum = w_true.sum() + w_false.sum()\n",
    "\n",
    "    w_true /= total_weights_sum\n",
    "    w_false /= total_weights_sum\n",
    "\n",
    "    # Создаем словарь для быстрого присвоения веса\n",
    "    weight_map_true = w_true.to_dict()\n",
    "    weight_map_false = w_false.to_dict()\n",
    "\n",
    "    # Функция для присвоения веса по строке\n",
    "    def assign_weight(row):\n",
    "        if row[label_col] == 1:\n",
    "            return weight_map_true.get(row[dataset_col], 0)\n",
    "        else:\n",
    "            return weight_map_false.get(row[dataset_col], 0)\n",
    "\n",
    "    weights = df.apply(assign_weight, axis=1)\n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ячейка 10.1: Вычисление весов\n",
    "reduced_train_df['weight'] = compute_sample_weights(reduced_train_df)\n",
    "valid_df['weight'] = compute_sample_weights(valid_df)\n",
    "test_df['weight'] = compute_sample_weights(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u3p_W-InG79U"
   },
   "outputs": [],
   "source": [
    "# Ячейка 11: Токенизатор\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ao9UU2kkG-ZE"
   },
   "outputs": [],
   "source": [
    "# Ячейка 12.1: Функция чанкования, класс датасета и функция сбора.\n",
    "def process_one_doc(args):\n",
    "    doc_id, text, label, sample_weight, max_chunk_len, stride = args\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    if len(tokens) <= max_chunk_len:\n",
    "        num_chunks = 1\n",
    "        enc = tokenizer.encode_plus(\n",
    "            tokens,\n",
    "            is_split_into_words=True,\n",
    "            max_length=max_chunk_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        chunks = [{\n",
    "            'doc_id': doc_id,\n",
    "            'num_chunks': num_chunks,\n",
    "            'input_ids': enc['input_ids'].squeeze(0),\n",
    "            'attention_mask': enc['attention_mask'].squeeze(0),\n",
    "            'label': label,\n",
    "            'sample_weight': sample_weight\n",
    "        }]\n",
    "    else:\n",
    "        num_chunks = (len(tokens) - max_chunk_len) // stride + 1\n",
    "        chunks = []\n",
    "        for i in range(0, len(tokens), stride):\n",
    "            chunk_tokens = tokens[i:i+max_chunk_len]\n",
    "            if len(chunk_tokens) == 0:\n",
    "                break\n",
    "            enc = tokenizer.encode_plus(\n",
    "                chunk_tokens,\n",
    "                is_split_into_words=True,\n",
    "                max_length=max_chunk_len,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            chunks.append({\n",
    "                'doc_id': doc_id,\n",
    "                'num_chunks': num_chunks,\n",
    "                'input_ids': enc['input_ids'].squeeze(0),\n",
    "                'attention_mask': enc['attention_mask'].squeeze(0),\n",
    "                'label': label,\n",
    "                'sample_weight': sample_weight\n",
    "            })\n",
    "    return chunks\n",
    "\n",
    "def process_chunking(df, max_chunk_len, stride, text_column, label_column, weight_column, num_workers=4):\n",
    "    args = [(idx, row[text_column], row[label_column], row[weight_column], max_chunk_len, stride)\n",
    "            for idx, row in df.iterrows()]\n",
    "    \n",
    "    chunks_list = [None] * len(args)\n",
    "    with ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
    "        futures = {executor.submit(process_one_doc, arg): i for i, arg in enumerate(args)}\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Chunking parallel\"):\n",
    "            idx = futures[future]\n",
    "            chunks_list[idx] = future.result()\n",
    "    \n",
    "    new_df = df.copy()\n",
    "    new_df['chunks'] = chunks_list\n",
    "    return new_df\n",
    "\n",
    "\n",
    "class ChunkedTextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, chunks):\n",
    "        self.chunks = chunks\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.chunks)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.chunks[idx]\n",
    "        return chunk\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "    labels = torch.tensor([item['label'] for item in batch], dtype=torch.long)\n",
    "    doc_ids = torch.tensor([item['doc_id'] for item in batch], dtype=torch.long)\n",
    "    num_chunks = torch.tensor([item['num_chunks'] for item in batch], dtype=torch.long)\n",
    "    sample_weights = torch.tensor([item['sample_weight'] for item in batch], dtype=torch.float)\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': labels,\n",
    "        'doc_ids': doc_ids,\n",
    "        'num_chunks': num_chunks,\n",
    "        'sample_weights': sample_weights\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ячейка 12.2: Формирование чанкованных датафреймов.\n",
    "chunked_core_train_df = process_chunking(df=core_train_df.drop(columns=['dataset_name']), max_chunk_len=config.MAX_LEN, stride=config.MAX_LEN//2, text_column='statement', label_column='label_bin', weight_column='weight')\n",
    "chunked_train_df = process_chunking(df=reduced_train_df.drop(columns=['dataset_name']), max_chunk_len=config.MAX_LEN, stride=config.MAX_LEN//2, text_column='statement', label_column='label_bin', weight_column='weight')\n",
    "chunked_valid_df = process_chunking(df=valid_df.drop(columns=['dataset_name']), max_chunk_len=config.MAX_LEN, stride=config.MAX_LEN//2, text_column='statement', label_column='label_bin', weight_column='weight')\n",
    "chunked_test_df  = process_chunking(df=test_df.drop(columns=['dataset_name']), max_chunk_len=config.MAX_LEN, stride=config.MAX_LEN//2, text_column='statement', label_column='label_bin', weight_column='weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chunked_train_df['chunks'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks_to_list(df: pd.DataFrame):\n",
    "    all_list = []\n",
    "    for idx, row in df.iterrows():\n",
    "        all_list.extend(row['chunks'])\n",
    "    return all_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ячейка 13: Формирование загрузчиков данных.\n",
    "core_train_loader_bin = DataLoader(ChunkedTextDataset(chunks_to_list(chunked_core_train_df)), batch_size=config.BATCH_SIZE_VALID, shuffle=False, num_workers=config.NUM_WORKERS, collate_fn=collate_fn)\n",
    "valid_loader_bin = DataLoader(ChunkedTextDataset(chunks_to_list(chunked_valid_df)), batch_size=config.BATCH_SIZE_VALID, shuffle=False, num_workers=config.NUM_WORKERS, collate_fn=collate_fn)\n",
    "test_loader_bin  = DataLoader(ChunkedTextDataset(chunks_to_list(chunked_test_df)), batch_size=config.BATCH_SIZE_VALID, shuffle=False, num_workers=config.NUM_WORKERS, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6tq9xD_0HEvD"
   },
   "outputs": [],
   "source": [
    "# Ячейка 14: Модель с выходом для любого количества классов\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, model_name, num_classes, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.config = AutoConfig.from_pretrained(model_name)\n",
    "        self.config.output_hidden_states = False\n",
    "        self.model = AutoModel.from_pretrained(model_name, config=self.config)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.classifier = nn.Linear(self.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_embeddings = outputs.last_hidden_state[:, 0]  # CLS токен\n",
    "        cls_embeddings = self.dropout(cls_embeddings)\n",
    "        logits = self.classifier(cls_embeddings)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FYdsSs34OY69"
   },
   "outputs": [],
   "source": [
    "# Ячейка 15: EarlyStopping\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, score):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            return False\n",
    "        elif score < self.best_score + self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                return True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ячейка 16.1: Тренировочный цикл, бинарная классификация Правда/Ложь.\n",
    "# Ctrl+F8 - выполнить все ячейки выше\n",
    "\n",
    "model_bin = CustomModel(config.MODEL, config.NUM_CLASSES_BIN, 0.3).to(device)\n",
    "optimizer = torch.optim.AdamW(model_bin.parameters(), lr=config.LEARNING_RATE, weight_decay=config.WEIGHT_DECAY_BIN)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2)\n",
    "criterion = nn.BCEWithLogitsLoss(reduction='none')\n",
    "scaler = torch.amp.GradScaler()\n",
    "\n",
    "truth_threshold = 0.5\n",
    "early_stopping = EarlyStopping(patience=3, min_delta=1e-4)\n",
    "\n",
    "model_save_path = os.path.join(output_dir, \"deberta_v3_xsmall_binary_chunked_extended.pth\")\n",
    "best_macro_f1 = 0\n",
    "best_threshold = truth_threshold\n",
    "thresholds_to_try = np.linspace(0.1, 0.9, 81)\n",
    "\n",
    "def find_best_threshold(targets, preds_probs, thresholds):\n",
    "    best_f1 = 0\n",
    "    best_t = thresholds[0]\n",
    "    for t in thresholds:\n",
    "        preds_bin = (preds_probs > t).astype(int)\n",
    "        f1 = f1_score(targets, preds_bin, average='macro')\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_t = t\n",
    "    return best_t, best_f1\n",
    "\n",
    "def aggregate_predictions(model, data_loader):\n",
    "    model.eval()\n",
    "    val_chunk_logits = []\n",
    "    val_chunk_doc_ids = []\n",
    "    val_labels_dict = {}\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Validation\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device).float().unsqueeze(1)\n",
    "            doc_ids = batch['doc_ids']\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            val_chunk_logits.append(outputs.detach().cpu())\n",
    "            val_chunk_doc_ids.append(doc_ids)\n",
    "            for doc_id, label in zip(doc_ids.tolist(), labels.detach().cpu().tolist()):\n",
    "                if doc_id not in val_labels_dict:\n",
    "                    val_labels_dict[doc_id] = label\n",
    "    chunk_logits_tensor = torch.cat(val_chunk_logits, dim=0)\n",
    "    chunk_doc_ids_total = torch.cat(val_chunk_doc_ids, dim=0)\n",
    "    doc_logits_dict = defaultdict(list)\n",
    "    for logit, doc_id in zip(chunk_logits_tensor, chunk_doc_ids_total):\n",
    "        doc_logits_dict[doc_id.item()].append(logit)\n",
    "    val_preds = []\n",
    "    val_targets = []\n",
    "    for doc_id, logits_list in doc_logits_dict.items():\n",
    "        logits_stack = torch.stack(logits_list)\n",
    "        agg_logit = logits_stack.mean(dim=0).item()\n",
    "        val_preds.append(agg_logit)\n",
    "        val_targets.append(val_labels_dict[doc_id])\n",
    "    return np.array(val_targets), np.array(val_preds)\n",
    "\n",
    "def chunks_to_list(df: pd.DataFrame):\n",
    "    all_list = []\n",
    "    for idx, row in df.iterrows():\n",
    "        all_list.extend(row['chunks'])\n",
    "    return all_list\n",
    "\n",
    "# # Первая тренировка на core_train_loader_bin\n",
    "# print(\"Initial training on core_train_loader_bin...\")\n",
    "# model_bin.train()\n",
    "# for epoch in range(config.EPOCHS):\n",
    "#     optimizer.zero_grad(set_to_none=True)\n",
    "#     for batch in tqdm(core_train_loader_bin, desc=f\"Initial Training Epoch {epoch+1}\"):\n",
    "#         input_ids = batch['input_ids'].to(device)\n",
    "#         attention_mask = batch['attention_mask'].to(device)\n",
    "#         labels = batch['labels'].to(device).float().unsqueeze(1)\n",
    "#         sample_weights = batch['sample_weights'].to(device).unsqueeze(1)\n",
    "#         with torch.amp.autocast(device_type=\"cuda\"):\n",
    "#             outputs = model_bin(input_ids, attention_mask)\n",
    "#             losses = criterion(outputs, labels)\n",
    "#             weighted_losses = losses * sample_weights\n",
    "#             loss = weighted_losses.mean() / config.GRADIENT_ACCUMULATION_STEPS\n",
    "#         scaler.scale(loss).backward()\n",
    "#         scaler.step(optimizer)\n",
    "#         scaler.update()\n",
    "#         optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "iteration = 0\n",
    "# max_iterations = 10\n",
    "\n",
    "# while iteration < max_iterations:\n",
    "while True:\n",
    "    print(f\"Iteration {iteration + 1}: Validation on full training set\")\n",
    "\n",
    "    train_loader_bin = DataLoader(\n",
    "        ChunkedTextDataset(chunks_to_list(chunked_train_df)),\n",
    "        batch_size=config.BATCH_SIZE_VALID,\n",
    "        shuffle=False,\n",
    "        num_workers=config.NUM_WORKERS,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    val_targets, val_preds = aggregate_predictions(model_bin, train_loader_bin)\n",
    "\n",
    "    probs = 1 / (1 + np.exp(-val_preds))\n",
    "    current_best_threshold, current_macro_f1 = find_best_threshold(val_targets, probs, thresholds_to_try)\n",
    "\n",
    "    print(f\"Best threshold={current_best_threshold:.3f}, Macro F1={current_macro_f1:.4f}\")\n",
    "\n",
    "    if current_macro_f1 > best_macro_f1:\n",
    "        best_macro_f1 = current_macro_f1\n",
    "        best_threshold = current_best_threshold\n",
    "        torch.save({\n",
    "            'model_state_dict': model_bin.state_dict(),\n",
    "            'best_threshold': best_threshold\n",
    "        }, model_save_path)\n",
    "        print(f\"Saved new best model\")\n",
    "\n",
    "    abs_diff = np.abs(val_targets.squeeze() - val_preds)\n",
    "    retrain_mask = abs_diff > 0.2\n",
    "    retrain_df = chunked_train_df.iloc[retrain_mask.nonzero()[0]].copy()\n",
    "\n",
    "    print(f\"Examples selected for retraining: {len(retrain_df)}\")\n",
    "\n",
    "    if len(retrain_df) == 0:\n",
    "        print(\"No hard examples left, stopping training.\")\n",
    "        break\n",
    "\n",
    "    # Обновляем doc_id для чанков\n",
    "    for idx, row in retrain_df.iterrows():\n",
    "        for chunk in row['chunks']:\n",
    "            chunk['doc_id'] = idx\n",
    "\n",
    "    train_loader_retrain = DataLoader(\n",
    "        ChunkedTextDataset(chunks_to_list(retrain_df)),\n",
    "        batch_size=config.BATCH_SIZE_VALID,\n",
    "        shuffle=True,\n",
    "        num_workers=config.NUM_WORKERS,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    model_bin.train()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    for step, batch in enumerate(tqdm(train_loader_retrain, desc=f\"Retraining iteration {iteration + 1}\")):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device).float().unsqueeze(1)\n",
    "        sample_weights = batch['sample_weights'].to(device).unsqueeze(1)\n",
    "        with torch.amp.autocast(device_type=\"cuda\"):\n",
    "            outputs = model_bin(input_ids, attention_mask)\n",
    "            losses = criterion(outputs, labels)\n",
    "            weighted_losses = losses * sample_weights\n",
    "            loss = weighted_losses.mean() / config.GRADIENT_ACCUMULATION_STEPS\n",
    "        scaler.scale(loss).backward()\n",
    "        if (step + 1) % config.GRADIENT_ACCUMULATION_STEPS == 0 or (step + 1) == len(train_loader_retrain):\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    iteration += 1\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fQ6-3SFzH7UW"
   },
   "outputs": [],
   "source": [
    "# Ячейка 16.2: Корректировка модельных вероятностей.\n",
    "\n",
    "model_bin = CustomModel(config.MODEL, config.NUM_CLASSES_BIN, 0.3).to(device)\n",
    "\n",
    "# Загрузка лучшей модели и порога\n",
    "checkpoint = torch.load(os.path.join(output_dir, \"deberta_v3_xsmall_binary_chunked_extended.pth\"), weights_only=False)\n",
    "model_bin.load_state_dict(checkpoint['model_state_dict'])\n",
    "best_threshold = checkpoint['best_threshold']\n",
    "print(f\"Загрузили модель с порогом {best_threshold:.3f}\")\n",
    "\n",
    "# Валидация\n",
    "model_bin.eval()\n",
    "val_chunk_logits = []\n",
    "val_chunk_doc_ids = []\n",
    "val_labels_dict = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(valid_loader_bin, desc=\"Validation\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device).float().unsqueeze(1)\n",
    "        doc_ids = batch['doc_ids']\n",
    "\n",
    "        outputs = model_bin(input_ids, attention_mask)\n",
    "        val_chunk_logits.append(outputs.detach().cpu())\n",
    "        val_chunk_doc_ids.append(doc_ids)\n",
    "\n",
    "        for doc_id, label in zip(doc_ids.tolist(), labels.detach().cpu().tolist()):\n",
    "            if doc_id not in val_labels_dict:\n",
    "                val_labels_dict[doc_id] = label\n",
    "\n",
    "chunk_logits_tensor = torch.cat(val_chunk_logits, dim=0)\n",
    "chunk_doc_ids_total = torch.cat(val_chunk_doc_ids, dim=0)\n",
    "doc_logits_dict = defaultdict(list)\n",
    "\n",
    "for logit, doc_id in zip(chunk_logits_tensor, chunk_doc_ids_total):\n",
    "    doc_logits_dict[doc_id.item()].append(logit)\n",
    "\n",
    "val_preds = []\n",
    "val_targets = []\n",
    "\n",
    "for doc_id, logits_list in doc_logits_dict.items():\n",
    "    logits_stack = torch.stack(logits_list)\n",
    "    agg_logit = logits_stack.mean(dim=0).item()\n",
    "    val_preds.append(agg_logit)\n",
    "    val_targets.append(val_labels_dict[doc_id])\n",
    "\n",
    "val_preds = np.array(val_preds)\n",
    "val_targets = np.array(val_targets)\n",
    "val_probs = 1 / (1 + np.exp(-val_preds))\n",
    "\n",
    "# Подбор оптимального порога по F1 для неоткалиброванных вероятностей\n",
    "precision, recall, thresholds = precision_recall_curve(val_targets, val_probs)\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "best_idx = np.argmax(f1_scores)\n",
    "best_threshold = thresholds[best_idx]\n",
    "best_f1 = f1_scores[best_idx]\n",
    "\n",
    "print(f\"Uncalibrated best threshold: {best_threshold:.4f}, Macro F1: {best_f1:.4f}\")\n",
    "\n",
    "val_probs = np.array(val_probs).flatten()\n",
    "val_targets = np.array(val_targets).flatten()\n",
    "\n",
    "# Обучаем калибровщики\n",
    "platt = LogisticRegression()\n",
    "platt.fit(val_probs.reshape(-1, 1), val_targets)\n",
    "\n",
    "iso_reg = IsotonicRegression(out_of_bounds='clip')\n",
    "iso_reg.fit(val_probs, val_targets)\n",
    "\n",
    "# Применяем калибровку\n",
    "val_probs_platt = platt.predict_proba(val_probs.reshape(-1, 1))[:, 1]\n",
    "val_probs_iso = iso_reg.predict(val_probs)\n",
    "\n",
    "def find_best_threshold(probs, targets):\n",
    "    precision, recall, thresholds = precision_recall_curve(targets, probs)\n",
    "    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "    best_idx = np.argmax(f1_scores)\n",
    "    return thresholds[best_idx], f1_scores[best_idx]\n",
    "\n",
    "best_thresh_platt, best_f1_platt = find_best_threshold(val_probs_platt, val_targets)\n",
    "best_thresh_iso, best_f1_iso = find_best_threshold(val_probs_iso, val_targets)\n",
    "\n",
    "print(f\"Platt scaling: best threshold = {best_thresh_platt:.4f}, best macro F1 = {best_f1_platt:.4f}\")\n",
    "print(f\"Isotonic regression: best threshold = {best_thresh_iso:.4f}, best macro F1 = {best_f1_iso:.4f}\")\n",
    "\n",
    "def classification_report_with_threshold(probs, targets, threshold):\n",
    "    preds = (probs > threshold).astype(int)\n",
    "    print(classification_report(targets, preds, digits=4, zero_division=0))\n",
    "    print(\"Confusion matrix:\")\n",
    "    print(confusion_matrix(targets, preds))\n",
    "\n",
    "print(\"\\nUncalibrated classification report:\")\n",
    "classification_report_with_threshold(val_probs, val_targets, best_threshold)\n",
    "\n",
    "print(\"\\nPlatt scaling classification report:\")\n",
    "classification_report_with_threshold(val_probs_platt, val_targets, best_thresh_platt)\n",
    "\n",
    "print(\"\\nIsotonic regression classification report:\")\n",
    "classification_report_with_threshold(val_probs_iso, val_targets, best_thresh_iso)\n",
    "\n",
    "# Сохраняем калибровщики для теста\n",
    "joblib.dump(platt, os.path.join(output_dir, \"platt_scaler_chunked_extended.pkl\"))\n",
    "joblib.dump(iso_reg, os.path.join(output_dir, \"isotonic_regressor_chunked_extended.pkl\"))\n",
    "\n",
    "# Сохраняем оптимальные вероятности калибровщиков тоже\n",
    "\n",
    "thresholds_dict = {\n",
    "    \"platt_threshold\": float(best_thresh_platt),\n",
    "    \"isotonic_threshold\": float(best_thresh_iso)\n",
    "}\n",
    "\n",
    "with open(os.path.join(output_dir, \"calibration_thresholds_chunked_extended.json\"), \"w\") as f:\n",
    "    json.dump(thresholds_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "enninm2tGzzP"
   },
   "outputs": [],
   "source": [
    "# Ячейка 17.1: Тестирование результата (тестовые данные из датасета)\n",
    "\n",
    "# Загрузка модели и порога\n",
    "model_bin = CustomModel(config.MODEL, config.NUM_CLASSES_BIN, 0.3).to(device)\n",
    "checkpoint = torch.load(os.path.join(output_dir, \"deberta_v3_xsmall_binary_chunked_extended.pth\"), weights_only=False)\n",
    "model_bin.load_state_dict(checkpoint['model_state_dict'])\n",
    "truth_threshold = checkpoint['best_threshold']\n",
    "print(f\"Загрузили модель с порогом {truth_threshold:.3f}\")\n",
    "\n",
    "model_bin.eval()\n",
    "\n",
    "\n",
    "# Загружаем калибровщики\n",
    "platt = joblib.load(os.path.join(output_dir, \"platt_scaler_chunked_extended.pkl\"))\n",
    "iso_reg = joblib.load(os.path.join(output_dir, \"isotonic_regressor_chunked_extended.pkl\"))\n",
    "\n",
    "# Загружаем пороги\n",
    "with open(os.path.join(output_dir, \"calibration_thresholds_chunked_extended.json\"), \"r\") as f:\n",
    "    thresholds = json.load(f)\n",
    "\n",
    "platt_threshold = thresholds[\"platt_threshold\"]\n",
    "isotonic_threshold = thresholds[\"isotonic_threshold\"]\n",
    "\n",
    "\n",
    "# calibrator - platt или iso_reg, truth_threshold - platt_*/isotonic_*\n",
    "calibrator = platt\n",
    "truth_threshold = platt_threshold\n",
    "\n",
    "all_chunk_logits = []\n",
    "all_chunk_doc_ids = []\n",
    "all_labels_dict = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader_bin, desc=\"Testing\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        doc_ids = batch['doc_ids']\n",
    "        \n",
    "        labels = batch.get('labels')\n",
    "        if labels is not None:\n",
    "            labels = labels.to(device).float().unsqueeze(1)\n",
    "\n",
    "        outputs = model_bin(input_ids, attention_mask)\n",
    "        all_chunk_logits.append(outputs.detach().cpu())\n",
    "        all_chunk_doc_ids.append(doc_ids)\n",
    "        \n",
    "        if labels is not None:\n",
    "            for doc_id, label in zip(doc_ids.tolist(), labels.cpu().tolist()):\n",
    "                if doc_id not in all_labels_dict:\n",
    "                    all_labels_dict[doc_id] = label\n",
    "\n",
    "chunk_logits_tensor = torch.cat(all_chunk_logits, dim=0)\n",
    "chunk_doc_ids_total = torch.cat(all_chunk_doc_ids, dim=0)\n",
    "\n",
    "doc_logits_dict = defaultdict(list)\n",
    "for logit, doc_id in zip(chunk_logits_tensor, chunk_doc_ids_total):\n",
    "    doc_logits_dict[doc_id.item()].append(logit)\n",
    "\n",
    "test_preds = []\n",
    "test_targets = []\n",
    "test_probs = []\n",
    "\n",
    "for doc_id, logits_list in doc_logits_dict.items():\n",
    "    logits_stack = torch.stack(logits_list)\n",
    "    agg_logit = logits_stack.mean(dim=0).item()\n",
    "    prob_raw = 1 / (1 + np.exp(-agg_logit))\n",
    "\n",
    "    # Калибровка\n",
    "    if isinstance(calibrator, LogisticRegression):\n",
    "        prob_calibrated = calibrator.predict_proba(np.array([[prob_raw]]))[:, 1][0]\n",
    "    else:\n",
    "        prob_calibrated = calibrator.predict(np.array([prob_raw]))\n",
    "\n",
    "    test_probs.append(prob_calibrated)\n",
    "    pred = int(prob_calibrated > truth_threshold)\n",
    "    test_preds.append(pred)\n",
    "\n",
    "    if all_labels_dict:\n",
    "        test_targets.append(all_labels_dict[doc_id])\n",
    "\n",
    "if test_targets:\n",
    "    test_targets = np.array(test_targets)\n",
    "    test_preds = np.array(test_preds)\n",
    "    test_probs = np.array(test_probs)\n",
    "\n",
    "    print(\"Test Classification Report:\")\n",
    "    print(classification_report(test_targets, test_preds, digits=4, zero_division=0))\n",
    "    print(\"Macro F1-score:\", f1_score(test_targets, test_preds, average='macro'))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(test_targets, test_preds))\n",
    "\n",
    "    # Индексы ошибок\n",
    "    false_positives_idx = np.where((test_preds == 1) & (test_targets == 0))[0]\n",
    "    false_negatives_idx = np.where((test_preds == 0) & (test_targets == 1))[0]\n",
    "\n",
    "    # Данные ошибок и их уверенности\n",
    "    fp_confidence = test_probs[false_positives_idx]\n",
    "    fn_confidence = test_probs[false_negatives_idx]\n",
    "\n",
    "    # Сортируем по убыванию уверенности\n",
    "    fp_sorted_idx = false_positives_idx[np.argsort(-fp_confidence)]\n",
    "    fn_sorted_idx = false_negatives_idx[np.argsort(-fn_confidence)]\n",
    "\n",
    "    print(\"False Positives (ложь -> правда), отсортированные по уверенности ошибки:\")\n",
    "    for i in fp_sorted_idx[:5]:\n",
    "        print(f\"Текст: {test_df.iloc[i]['statement']}\")\n",
    "        print(f\"Вероятность (ошибки): {test_probs[i]:.4f}\")\n",
    "        print(\"---\")\n",
    "\n",
    "    print(\"False Negatives (правда -> ложь), отсортированные по уверенности ошибки:\")\n",
    "    for i in fn_sorted_idx[:5]:\n",
    "        print(f\"Текст: {test_df.iloc[i]['statement']}\")\n",
    "        print(f\"Вероятность (ошибки): {test_probs[i]:.4f}\")\n",
    "        print(\"---\")\n",
    "else:\n",
    "    print(\"Метки отсутствуют, выведены только предсказания.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "eHmdBxNGmo0z"
   },
   "outputs": [],
   "source": [
    "# Ячейка 17.2: Тестирование результата (ручное)\n",
    "test_device = torch.device('cpu')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.MODEL)\n",
    "\n",
    "# Загрузка модели и порога\n",
    "model_bin = CustomModel(config.MODEL, config.NUM_CLASSES_BIN, 0.3).to(test_device)\n",
    "checkpoint = torch.load(os.path.join(output_dir, \"deberta_v3_xsmall_binary_chunked_extended.pth\"), weights_only=False)\n",
    "model_bin.load_state_dict(checkpoint['model_state_dict'])\n",
    "truth_threshold = checkpoint['best_threshold']\n",
    "\n",
    "\n",
    "# Загружаем калибровщики\n",
    "platt = joblib.load(os.path.join(output_dir, \"platt_scaler_chunked_extended.pkl\"))\n",
    "iso_reg = joblib.load(os.path.join(output_dir, \"isotonic_regressor_chunked_extended.pkl\"))\n",
    "\n",
    "# Загружаем пороги\n",
    "with open(os.path.join(output_dir, \"calibration_thresholds_chunked_extended.json\"), \"r\") as f:\n",
    "    thresholds = json.load(f)\n",
    "\n",
    "platt_threshold = thresholds[\"platt_threshold\"]\n",
    "isotonic_threshold = thresholds[\"isotonic_threshold\"]\n",
    "\n",
    "\n",
    "# calibrator - platt/iso_reg, truth_threshold - platt_*/isotonic_*\n",
    "calibrator = platt\n",
    "truth_threshold = platt_threshold\n",
    "\n",
    "\n",
    "\n",
    "# Функция для разбиения текста на чанки\n",
    "def chunk_text(text, tokenizer, max_len=config.MAX_LEN, stride=config.MAX_LEN//2):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), stride):\n",
    "        chunk_tokens = tokens[i:i+max_len]\n",
    "        if not chunk_tokens:\n",
    "            break\n",
    "        enc = tokenizer.encode_plus(\n",
    "            chunk_tokens,\n",
    "            is_split_into_words=True,\n",
    "            max_length=max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        chunks.append(enc)\n",
    "    return chunks\n",
    "\n",
    "# Функция предсказания с калибровкой, усредняя чанки\n",
    "def predict_text_chunked(text, model, tokenizer, calibrator, threshold, device):\n",
    "    model.eval()\n",
    "    chunks = chunk_text(text, tokenizer)\n",
    "    input_ids = torch.cat([chunk['input_ids'] for chunk in chunks], dim=0).to(device)\n",
    "    attention_mask = torch.cat([chunk['attention_mask'] for chunk in chunks], dim=0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        logits = logits.squeeze(-1).cpu()\n",
    "\n",
    "    agg_logit = logits.mean().item()\n",
    "    prob_raw = 1 / (1 + np.exp(-agg_logit))\n",
    "\n",
    "    if isinstance(calibrator, LogisticRegression):\n",
    "        prob_calibrated = calibrator.predict_proba(np.array([[prob_raw]]))[0, 1]\n",
    "    else:\n",
    "        prob_calibrated = calibrator.predict(np.array([prob_raw]))\n",
    "\n",
    "    label = \"Правда\" if prob_calibrated > threshold else \"Ложь\"\n",
    "    return label, prob_calibrated\n",
    "\n",
    "# Функция для SHAP\n",
    "def predict_proba_shap(texts):\n",
    "    probs = []\n",
    "    for text in texts:\n",
    "        _, prob = predict_text_chunked(text, model_bin, tokenizer, calibrator, truth_threshold, test_device)\n",
    "        probs.append(prob)\n",
    "    probs = np.array(probs)\n",
    "    return np.stack([1 - probs, probs], axis=1)\n",
    "\n",
    "shap_explainer = shap.Explainer(predict_proba_shap, masker=shap.maskers.Text(tokenizer))\n",
    "\n",
    "print(\"Введите текст для классификации (пустая строка для выхода):\")\n",
    "\n",
    "translator = Translator()\n",
    "\n",
    "standart_phrases = [\"Вчерашний шторм был самым слабым за последние годы, он нанес катастрофический ущерб всему городу.\",\n",
    "                    \"Этот абсолютно безопасный метод лечения имеет смертельно опасные побочные эффекты.\",\n",
    "                    \"Компания одновременно побила все рекорды по прибыли и обанкротилась в этом квартале.\",\n",
    "                    \"Ученые обнаружили кота, который одновременно и жив, и мертв.\",\n",
    "                    \"Новое устройство производит больше энергии, чем потребляет, создавая вечный двигатель.\",\n",
    "                    \"В результате аварии пять человек погибли, и еще трое получили серьезные ранения, но жертв удалось избежать.\",\n",
    "                    \"Президент скоропостижно скончался.\",\n",
    "                    \"Глава государства внезапно умер.\",\n",
    "                    \"Лидер нации нашел свой конец.\",\n",
    "                    \"Земля вращается вокруг Солнца.\",\n",
    "                    \"Солнце вращается вокруг Земли.\",\n",
    "                    \"Этот ресторан подает самую отвратительную еду в городе.\",\n",
    "                    \"Если бы мы все перешли на электромобили, загрязнение воздуха исчезло бы за один день.\",\n",
    "                    \"Правда ли, что правительство скрывает инопланетян?\",\n",
    "                    \"Новый завод обеспечит работу для 200% безработных в регионе.\",\n",
    "                    \"Рождаемость в городе выросла на 150% за одну ночь.\",\n",
    "                    \"Цена на хлеб упала до -5 долларов.\",\n",
    "                    \"Комитет единогласно одобрил законопроект.\",\n",
    "                    \"Комитет единогласно отклонил законопроект.\",\n",
    "                    \"Это общеизвестныйфакт.\",\n",
    "                    \"Вакцины вызывают @втизм.\",\n",
    "                    \"Чтобы приготовить яичницу, нужно сначала почистить апельсин.\",\n",
    "                    \"Рыбы вышли на берег прогуляться.\",\n",
    "                    \"Солнце сделано из жидкого шоколада.\"]\n",
    "\n",
    "standart_number = len(standart_phrases)\n",
    "while True:\n",
    "    if (standart_number < len(standart_phrases)):\n",
    "        raw_input = standart_phrases[standart_number]\n",
    "        standart_number = standart_number + 1\n",
    "    else:\n",
    "        raw_input = input(\">>> \").strip()\n",
    "        if raw_input == \"\":\n",
    "            print(\"Выход.\")\n",
    "            break\n",
    "    \n",
    "    print(f\"Оригинальный текст: {raw_input}\")\n",
    "\n",
    "    # Перевод текста\n",
    "    try:\n",
    "        translated_text = translator.translate(raw_input, dest='en').text\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка перевода: {e}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Перевод: {translated_text}\")\n",
    "\n",
    "    cleaned_text = clean_text(translated_text)\n",
    "\n",
    "    print(f\"Очищенный перевод: {cleaned_text}\")\n",
    "\n",
    "    label, prob = predict_text_chunked(cleaned_text, model_bin, tokenizer, calibrator, truth_threshold, test_device)\n",
    "    print(f\"Предсказание: {label} (вероятность: {prob:.4f})\")\n",
    "\n",
    "    # print(\"Объяснение SHAP:\")\n",
    "    # try:\n",
    "    #     shap_values = shap_explainer([cleaned_text])\n",
    "    #     shap.plots.text(shap_values)\n",
    "    # except Exception as e:\n",
    "    #     print(f\"Ошибка SHAP: {e}\")\n",
    "\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOCWHmRNEzc6Yso9DsyeBkZ",
   "gpuType": "T4",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1DzQigs4XnhASTWFz4tFmcowP7-7L1qCC",
     "timestamp": 1755436698395
    },
    {
     "file_id": "1xksvWYpsjixJL0vveot1zNuRH83VpFo3",
     "timestamp": 1754598838747
    },
    {
     "file_id": "12_vYSQnrud1k1AfUymQQglhBAD0WF5cL",
     "timestamp": 1751110204265
    },
    {
     "file_id": "1-ue3UfCLTYs_o2XAmjMQhG7jJnqlJ8-l",
     "timestamp": 1750716176166
    },
    {
     "file_id": "1bh2dI2F5EdXq8rgLn3BYXgmfAx_TUeyk",
     "timestamp": 1750453271234
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
