{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6UfwVwFaF4uY"
   },
   "outputs": [],
   "source": [
    "# Ячейка 1: Импорты и базовые настройки\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "import ast\n",
    "import copy\n",
    "import gc\n",
    "import itertools\n",
    "import joblib\n",
    "import json\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import scipy as sp\n",
    "import string\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, classification_report, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import tokenizers\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig, get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Установка устройства\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Current device is: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WBm7vuKrF-fA"
   },
   "outputs": [],
   "source": [
    "# Ячейка 2: Пути к файлам и конфигурация\n",
    "main_dir = os.getenv(\"MAIN_DIR\")\n",
    "data_dir = os.path.join(main_dir, \"liar2\")\n",
    "extra_data_dir = os.path.join(main_dir, \"liar-twitter\")\n",
    "output_dir = os.path.join(main_dir, \"output\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "class config:\n",
    "    MODEL = \"microsoft/deberta-v3-xsmall\"\n",
    "    MAX_LEN = 128\n",
    "    BATCH_SIZE_TRAIN = 32\n",
    "    BATCH_SIZE_VALID = 32\n",
    "    EPOCHS = 10\n",
    "    LEARNING_RATE = 2e-5\n",
    "    SEED = 42\n",
    "    NUM_CLASSES = 5  # меньше на 1, т.к. далее мы объединяем 0 и 1 классы\n",
    "    NUM_WORKERS = 0  # если мало памяти, лучше 0\n",
    "    GRADIENT_ACCUMULATION_STEPS = 4\n",
    "    WEIGHT_DECAY = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zww_twgFGUM_"
   },
   "outputs": [],
   "source": [
    "# Ячейка 3: Функция для установки seed\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything(config.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8nNPvM4XG4j6"
   },
   "outputs": [],
   "source": [
    "# Ячейка 4.1: Загрузка данных liar2\n",
    "train_df = pd.read_csv(os.path.join(data_dir, \"train.csv\"))\n",
    "valid_df = pd.read_csv(os.path.join(data_dir, \"valid.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(data_dir, \"test.csv\"))\n",
    "\n",
    "print(f\"Train shape liar2: {train_df.shape}\")\n",
    "print(f\"Valid shape liar2: {valid_df.shape}\")\n",
    "print(f\"Test shape liar2: {test_df.shape}\")\n",
    "display(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f2LIad2J0aeg"
   },
   "outputs": [],
   "source": [
    "# Ячейка 4.2: Загрузка данных liar-twitter\n",
    "extra_df = pd.read_csv(os.path.join(extra_data_dir, \"Liar_Dataset.csv\"))\n",
    "\n",
    "label_mapping = {\"pants-fire\": 0, \"FALSE\": 1, \"barely-true\": 2, \"half-true\": 3, \"mostly-true\": 4, \"TRUE\": 5}\n",
    "extra_df['label'] = extra_df['label'].map(label_mapping)\n",
    "\n",
    "extra_train_df, extra_test_valid_df = train_test_split(\n",
    "    extra_df,\n",
    "    test_size=0.2,\n",
    "    stratify=extra_df['label'],\n",
    "    random_state=config.SEED\n",
    ")\n",
    "\n",
    "extra_valid_df, extra_test_df = train_test_split(\n",
    "    extra_test_valid_df,\n",
    "    test_size=0.5,\n",
    "    stratify=extra_test_valid_df['label'],\n",
    "    random_state=config.SEED\n",
    ")\n",
    "\n",
    "print(f\"Train shape liar-twitter: {extra_train_df.shape}\")\n",
    "print(f\"Valid shape liar-twitter: {extra_valid_df.shape}\")\n",
    "print(f\"Test shape liar-twitter: {extra_test_df.shape}\")\n",
    "display(extra_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "se7oGZ4R57ll"
   },
   "outputs": [],
   "source": [
    "# Ячейка 4.3: Объединение датасетов\n",
    "train_df = train_df[['statement', 'label']]\n",
    "valid_df = valid_df[['statement', 'label']]\n",
    "test_df = test_df[['statement', 'label']]\n",
    "\n",
    "extra_train_df = extra_train_df[['statement', 'label']]\n",
    "extra_valid_df = extra_valid_df[['statement', 'label']]\n",
    "extra_test_df = extra_test_df[['statement', 'label']]\n",
    "\n",
    "train_df = pd.concat([train_df, extra_train_df], ignore_index=True)\n",
    "valid_df = pd.concat([valid_df, extra_valid_df], ignore_index=True)\n",
    "test_df = pd.concat([test_df, extra_test_df], ignore_index=True)\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(train_df['label'].value_counts())\n",
    "print(f\"Valid shape: {valid_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D0TV9rnBAYsl"
   },
   "outputs": [],
   "source": [
    "# Ячейка 5: Изменение данных и балансировка классов\n",
    "\n",
    "# Объединяем 0 (pants-on-fire) и 1 (false) классы.\n",
    "# Модель их плохо различает + они оба по сути ложные.\n",
    "# Эти классы описывают лишь наглость лжи и без\n",
    "# дополнительного контекста их будет сложно различить.\n",
    "train_df['label'] = train_df['label'].replace({0:1})\n",
    "valid_df['label'] = valid_df['label'].replace({0:1})\n",
    "test_df['label'] = test_df['label'].replace({0:1})\n",
    "\n",
    "# Теперь сдвигаем все классы на -1, чтобы классы начинались с 0\n",
    "train_df['label'] = train_df['label'] - 1\n",
    "valid_df['label'] = valid_df['label'] - 1\n",
    "test_df['label'] = test_df['label'] - 1\n",
    "\n",
    "# Андерсемплинг класса 0 до 20% от общего размера\n",
    "majority_class = 0\n",
    "max_samples = train_df['label'].value_counts().sort_values().iloc[len(train_df['label'].unique()) - 2]\n",
    "\n",
    "df_majority = train_df[train_df['label'] == majority_class]\n",
    "df_minority = train_df[train_df['label'] != majority_class]\n",
    "\n",
    "df_majority_downsampled = resample(df_majority,\n",
    "                                   replace=False,\n",
    "                                   n_samples=max_samples,\n",
    "                                   random_state=config.SEED)\n",
    "\n",
    "train_df_balanced = pd.concat([df_majority_downsampled, df_minority])\n",
    "\n",
    "# Оверсемплинг остальных классов до 15% от общего размера\n",
    "min_samples = round(sum(train_df['label'].value_counts())*0.15)\n",
    "\n",
    "dfs = []\n",
    "for label in train_df_balanced['label'].unique():\n",
    "    df_class = train_df_balanced[train_df_balanced['label'] == label]\n",
    "    if len(df_class) < min_samples:\n",
    "        df_upsampled = resample(df_class,\n",
    "                                replace=True,\n",
    "                                n_samples=min_samples,\n",
    "                                random_state=config.SEED)\n",
    "        dfs.append(df_upsampled)\n",
    "    else:\n",
    "        dfs.append(df_class)\n",
    "\n",
    "train_df_balanced = pd.concat(dfs).sample(frac=1, random_state=config.SEED).reset_index(drop=True)\n",
    "\n",
    "print(\"Баланс классов до ресэмплинга:\")\n",
    "print(train_df['label'].value_counts())\n",
    "\n",
    "print()\n",
    "print(f\"Андерсемплинг до: {max_samples}\")\n",
    "print(f\"Оверсемплинг до: {min_samples}\")\n",
    "print()\n",
    "\n",
    "print(\"Баланс классов после ресэмплинга:\")\n",
    "print(train_df_balanced['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u3p_W-InG79U"
   },
   "outputs": [],
   "source": [
    "# Ячейка 6: Токенизатор\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ao9UU2kkG-ZE"
   },
   "outputs": [],
   "source": [
    "# Ячейка 7: Dataset для многоклассовой классификации\n",
    "class LiarDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.texts = df['statement'].fillna(\"\").values\n",
    "        self.labels = df['label'].values\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(label, dtype=torch.long)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "epoFFPuQHBVF"
   },
   "outputs": [],
   "source": [
    "# Ячейка 8: Создаем датасеты и загрузчики\n",
    "train_dataset = LiarDataset(train_df_balanced, tokenizer, config.MAX_LEN)\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE_TRAIN, shuffle=True, num_workers=config.NUM_WORKERS)\n",
    "\n",
    "valid_dataset = LiarDataset(valid_df, tokenizer, config.MAX_LEN)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=config.BATCH_SIZE_VALID, shuffle=False, num_workers=config.NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6tq9xD_0HEvD"
   },
   "outputs": [],
   "source": [
    "# Ячейка 9: Модель с выходом для 6 классов\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, model_name, num_classes):\n",
    "        super().__init__()\n",
    "        self.config = AutoConfig.from_pretrained(model_name)\n",
    "        self.config.output_hidden_states = False\n",
    "        self.model = AutoModel.from_pretrained(model_name, config=self.config)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.classifier = nn.Linear(self.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state[:,0]  # CLS токен\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ECFNT6DkHJZf"
   },
   "outputs": [],
   "source": [
    "# Ячейка 10: Функции обучения и валидации\n",
    "def train_epoch(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    preds = []\n",
    "    targets = []\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        preds.append(outputs.argmax(dim=1).detach().cpu().numpy())\n",
    "        targets.append(labels.detach().cpu().numpy())\n",
    "\n",
    "    avg_loss = np.mean(losses)\n",
    "    preds = np.concatenate(preds)\n",
    "    targets = np.concatenate(targets)\n",
    "    acc = (preds == targets).mean()\n",
    "    return avg_loss, acc\n",
    "\n",
    "def valid_epoch(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    preds = []\n",
    "    targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Validation\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            preds.append(outputs.argmax(dim=1).detach().cpu().numpy())\n",
    "            targets.append(labels.detach().cpu().numpy())\n",
    "\n",
    "    avg_loss = np.mean(losses)\n",
    "    preds = np.concatenate(preds)\n",
    "    targets = np.concatenate(targets)\n",
    "    acc = (preds == targets).mean()\n",
    "    return avg_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4mQF3EvkHMg_"
   },
   "outputs": [],
   "source": [
    "# Ячейка 11: Основной тренировочный цикл\n",
    "model = CustomModel(config.MODEL, config.NUM_CLASSES).to(device)\n",
    "#model.load_state_dict(torch.load(os.path.join(output_dir, \"deberta_v3_xsmall_liar2.pth\")))\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.LEARNING_RATE, weight_decay=config.WEIGHT_DECAY)\n",
    "\n",
    "# Дисбаланс классов\n",
    "class_counts = train_df_balanced['label'].value_counts().sort_index()\n",
    "class_weights = 1.0 / torch.tensor(class_counts.values, dtype=torch.float)\n",
    "class_weights = class_weights / class_weights.sum()\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
    "\n",
    "# Без дисбаланса\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "looping = True\n",
    "\n",
    "while looping:\n",
    "  for epoch in range(config.EPOCHS):\n",
    "      model.train()\n",
    "      train_losses = []\n",
    "      train_preds = []\n",
    "      train_targets = []\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      for step, batch in enumerate(tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\")):\n",
    "          input_ids = batch['input_ids'].to(device)\n",
    "          attention_mask = batch['attention_mask'].to(device)\n",
    "          labels = batch['labels'].to(device)\n",
    "\n",
    "          with torch.cuda.amp.autocast():\n",
    "              outputs = model(input_ids, attention_mask)\n",
    "              loss = criterion(outputs, labels)\n",
    "              loss = loss / config.GRADIENT_ACCUMULATION_STEPS\n",
    "\n",
    "          scaler.scale(loss).backward()\n",
    "\n",
    "          if (step + 1) % config.GRADIENT_ACCUMULATION_STEPS == 0 or (step + 1) == len(train_loader):\n",
    "              scaler.step(optimizer)\n",
    "              scaler.update()\n",
    "              optimizer.zero_grad()\n",
    "\n",
    "          train_losses.append(loss.item() * config.GRADIENT_ACCUMULATION_STEPS)\n",
    "          preds = outputs.argmax(dim=1).detach().cpu().numpy()\n",
    "          train_preds.extend(preds)\n",
    "          train_targets.extend(labels.detach().cpu().numpy())\n",
    "\n",
    "      train_acc = np.mean(np.array(train_preds) == np.array(train_targets))\n",
    "      train_loss = np.mean(train_losses)\n",
    "\n",
    "      # Валидация (без градиентов)\n",
    "      model.eval()\n",
    "      val_losses = []\n",
    "      val_preds = []\n",
    "      val_targets = []\n",
    "\n",
    "      with torch.no_grad():\n",
    "          for batch in tqdm(valid_loader, desc=\"Validation\"):\n",
    "              input_ids = batch['input_ids'].to(device)\n",
    "              attention_mask = batch['attention_mask'].to(device)\n",
    "              labels = batch['labels'].to(device)\n",
    "\n",
    "              outputs = model(input_ids, attention_mask)\n",
    "              loss = criterion(outputs, labels)\n",
    "\n",
    "              val_losses.append(loss.item())\n",
    "              preds = outputs.argmax(dim=1).detach().cpu().numpy()\n",
    "              val_preds.extend(preds)\n",
    "              val_targets.extend(labels.detach().cpu().numpy())\n",
    "\n",
    "      val_acc = np.mean(np.array(val_preds) == np.array(val_targets))\n",
    "      val_loss = np.mean(val_losses)\n",
    "\n",
    "      print(f\"Epoch {epoch+1}/{config.EPOCHS} | Train loss: {train_loss:.4f} acc: {train_acc:.4f} | Val loss: {val_loss:.4f} acc: {val_acc:.4f}\")\n",
    "      print(\"Classification report (macro):\")\n",
    "      print(classification_report(val_targets, val_preds, digits=4, zero_division=0))\n",
    "      print(\"Macro F1-score:\", f1_score(val_targets, val_preds, average='macro'))\n",
    "      print(\"Confusion matrix:\")\n",
    "      print(confusion_matrix(val_targets, val_preds))\n",
    "      print(\"\")\n",
    "\n",
    "  user_input = input(\"Хотите продолжить тренировку? (yes/no): \").strip().upper()\n",
    "  if \"YES\" in user_input or \"Y\" in user_input:\n",
    "      looping = True\n",
    "  else:\n",
    "      looping = False\n",
    "\n",
    "# Сохранение модели\n",
    "torch.save(model.state_dict(), os.path.join(output_dir, \"deberta_v3_xsmall_liar2.pth\"))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNzne3wEy1y3V7HeGuRFksh",
   "gpuType": "T4",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
