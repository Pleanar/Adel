{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6UfwVwFaF4uY"
   },
   "outputs": [],
   "source": [
    "# Ячейка 1: Импорты и базовые настройки\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "import ast\n",
    "import copy\n",
    "import gc\n",
    "import itertools\n",
    "import joblib\n",
    "import json\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import scipy as sp\n",
    "import string\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, classification_report, f1_score, confusion_matrix, precision_recall_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import tokenizers\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig, get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Установка устройства\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Current device is: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WBm7vuKrF-fA"
   },
   "outputs": [],
   "source": [
    "# Ячейка 2: Пути к файлам и конфигурация\n",
    "main_dir = os.getenv(\"MAIN_DIR\")\n",
    "data_dir = os.path.join(main_dir, \"liar2\")\n",
    "extra_data_dir = os.path.join(main_dir, \"liar-twitter\")\n",
    "output_dir = os.path.join(main_dir, \"output\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "class config:\n",
    "    MODEL = \"microsoft/deberta-v3-xsmall\"\n",
    "    MAX_LEN = 128\n",
    "    BATCH_SIZE_TRAIN = 32\n",
    "    BATCH_SIZE_VALID = 32\n",
    "    EPOCHS = 20\n",
    "    LEARNING_RATE = 2e-5\n",
    "    LEARNING_RATE_BIN = 4e-6\n",
    "    SEED = 42\n",
    "    NUM_CLASSES = 4  # меньше на 1, т.к. далее мы объединяем 0 и 1 классы\n",
    "    NUM_CLASSES_BIN = 1  # для бинарной модели\n",
    "    NUM_WORKERS = 0  # если мало памяти, лучше 0\n",
    "    GRADIENT_ACCUMULATION_STEPS = 4\n",
    "    WEIGHT_DECAY = 0.01\n",
    "    WEIGHT_DECAY_BIN = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zww_twgFGUM_"
   },
   "outputs": [],
   "source": [
    "# Ячейка 3: Функция для установки seed\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything(config.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8nNPvM4XG4j6"
   },
   "outputs": [],
   "source": [
    "# Ячейка 4.1: Загрузка данных liar2 (https://huggingface.co/datasets/chengxuphd/liar2)\n",
    "train_df = pd.read_csv(os.path.join(data_dir, \"train.csv\"))\n",
    "valid_df = pd.read_csv(os.path.join(data_dir, \"valid.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(data_dir, \"test.csv\"))\n",
    "\n",
    "print(f\"Train shape liar2: {train_df.shape}\")\n",
    "print(f\"Valid shape liar2: {valid_df.shape}\")\n",
    "print(f\"Test shape liar2: {test_df.shape}\")\n",
    "display(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f2LIad2J0aeg"
   },
   "outputs": [],
   "source": [
    "# Ячейка 4.2: Загрузка данных liar-twitter (https://www.kaggle.com/datasets/muhammadimran112233/liar-twitter-dataset/data)\n",
    "extra_df = pd.read_csv(os.path.join(extra_data_dir, \"Liar_Dataset.csv\"))\n",
    "\n",
    "label_mapping = {\"pants-fire\": 0, \"FALSE\": 1, \"barely-true\": 2, \"half-true\": 3, \"mostly-true\": 4, \"TRUE\": 5}\n",
    "extra_df['label'] = extra_df['label'].map(label_mapping)\n",
    "\n",
    "extra_train_df, extra_test_valid_df = train_test_split(\n",
    "    extra_df,\n",
    "    test_size=0.2,\n",
    "    stratify=extra_df['label'],\n",
    "    random_state=config.SEED\n",
    ")\n",
    "\n",
    "extra_valid_df, extra_test_df = train_test_split(\n",
    "    extra_test_valid_df,\n",
    "    test_size=0.5,\n",
    "    stratify=extra_test_valid_df['label'],\n",
    "    random_state=config.SEED\n",
    ")\n",
    "\n",
    "print(f\"Train shape liar-twitter: {extra_train_df.shape}\")\n",
    "print(f\"Valid shape liar-twitter: {extra_valid_df.shape}\")\n",
    "print(f\"Test shape liar-twitter: {extra_test_df.shape}\")\n",
    "display(extra_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "se7oGZ4R57ll"
   },
   "outputs": [],
   "source": [
    "# Ячейка 4.3: Объединение датасетов\n",
    "train_df = train_df[['statement', 'label']]\n",
    "valid_df = valid_df[['statement', 'label']]\n",
    "test_df = test_df[['statement', 'label']]\n",
    "\n",
    "extra_train_df = extra_train_df[['statement', 'label']]\n",
    "extra_valid_df = extra_valid_df[['statement', 'label']]\n",
    "extra_test_df = extra_test_df[['statement', 'label']]\n",
    "\n",
    "train_df = pd.concat([train_df, extra_train_df], ignore_index=True)\n",
    "valid_df = pd.concat([valid_df, extra_valid_df], ignore_index=True)\n",
    "test_df = pd.concat([test_df, extra_test_df], ignore_index=True)\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(train_df['label'].value_counts())\n",
    "print(f\"Valid shape: {valid_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D0TV9rnBAYsl"
   },
   "outputs": [],
   "source": [
    "# Ячейка 5.1: Изменение данных\n",
    "\n",
    "# Объединяем 0 (pants-on-fire) и 1 (false) классы.\n",
    "# Модель их плохо различает + они оба по сути ложные.\n",
    "# Эти классы описывают лишь наглость лжи и без\n",
    "# дополнительного контекста их будет сложно различить.\n",
    "train_df['label'] = train_df['label'].replace({0:1})\n",
    "valid_df['label'] = valid_df['label'].replace({0:1})\n",
    "test_df['label'] = test_df['label'].replace({0:1})\n",
    "\n",
    "# Теперь сдвигаем все классы на -1, чтобы классы начинались с 0\n",
    "train_df['label'] = train_df['label'] - 1\n",
    "valid_df['label'] = valid_df['label'] - 1\n",
    "test_df['label'] = test_df['label'] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u49YbJUpkeAd"
   },
   "outputs": [],
   "source": [
    "# Ячейка 5.2: Балансировка классов\n",
    "# Андерсемплинг класса 0 до 20% от общего размера\n",
    "majority_class = 0\n",
    "max_samples = train_df['label'].value_counts().sort_values().iloc[len(train_df['label'].unique()) - 2]\n",
    "\n",
    "df_majority = train_df[train_df['label'] == majority_class]\n",
    "df_minority = train_df[train_df['label'] != majority_class]\n",
    "\n",
    "df_majority_downsampled = resample(df_majority,\n",
    "                                   replace=False,\n",
    "                                   n_samples=max_samples,\n",
    "                                   random_state=config.SEED)\n",
    "\n",
    "train_df_balanced = pd.concat([df_majority_downsampled, df_minority])\n",
    "\n",
    "# Оверсемплинг остальных классов до 15% от общего размера\n",
    "min_samples = round(sum(train_df['label'].value_counts())*0.15)\n",
    "\n",
    "dfs = []\n",
    "for label in train_df_balanced['label'].unique():\n",
    "    df_class = train_df_balanced[train_df_balanced['label'] == label]\n",
    "    if len(df_class) < min_samples:\n",
    "        df_upsampled = resample(df_class,\n",
    "                                replace=True,\n",
    "                                n_samples=min_samples,\n",
    "                                random_state=config.SEED)\n",
    "        dfs.append(df_upsampled)\n",
    "    else:\n",
    "        dfs.append(df_class)\n",
    "\n",
    "train_df_balanced = pd.concat(dfs).sample(frac=1, random_state=config.SEED).reset_index(drop=True)\n",
    "\n",
    "print(\"Баланс классов до ресэмплинга:\")\n",
    "print(train_df['label'].value_counts())\n",
    "\n",
    "print()\n",
    "print(f\"Андерсемплинг до: {max_samples}\")\n",
    "print(f\"Оверсемплинг до: {min_samples}\")\n",
    "print()\n",
    "\n",
    "print(\"Баланс классов после ресэмплинга:\")\n",
    "print(train_df_balanced['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5qUyniPeKFYs"
   },
   "outputs": [],
   "source": [
    "# Ячейка 5.3: Выделение данных под бинарную модель и балансировка\n",
    "train_df_balanced_bin = train_df_balanced.copy()\n",
    "train_df_balanced_bin['label_bin'] = train_df_balanced['label'].apply(lambda x: 1 if x == 4 else 0 if x == 0 else 2)\n",
    "train_df_balanced_bin = train_df_balanced_bin[train_df_balanced_bin['label_bin'] != 2].reset_index(drop=True)\n",
    "train_df_balanced_bin.drop(columns=['label'])\n",
    "valid_df_bin = valid_df.copy()\n",
    "valid_df_bin['label_bin'] = valid_df['label'].apply(lambda x: 1 if x == 4 else 0 if x == 0 else 2)\n",
    "valid_df_bin = valid_df_bin[valid_df_bin['label_bin'] != 2].reset_index(drop=True)\n",
    "valid_df_bin.drop(columns=['label'])\n",
    "test_df_bin = test_df.copy()\n",
    "test_df_bin['label_bin'] = test_df['label'].apply(lambda x: 1 if x == 4 else 0 if x == 0 else 2)\n",
    "test_df_bin = test_df_bin[test_df_bin['label_bin'] != 2].reset_index(drop=True)\n",
    "test_df_bin.drop(columns=['label'])\n",
    "\n",
    "# Делаем андерсемпл неправды для бинарной модели\n",
    "# df_class_0 = train_df_balanced_bin[train_df_balanced_bin['label_bin'] == 0]\n",
    "# df_class_1 = train_df_balanced_bin[train_df_balanced_bin['label_bin'] == 1]\n",
    "\n",
    "# n_samples = len(df_class_0)\n",
    "\n",
    "# df_class_1_downsampled = resample(df_class_1,\n",
    "#                                  replace=False,\n",
    "#                                  n_samples=n_samples,\n",
    "#                                  random_state=config.SEED)\n",
    "\n",
    "# train_df_balanced_bin = pd.concat([df_class_0, df_class_1_downsampled]).reset_index(drop=True)\n",
    "\n",
    "# Для вероятностной модели фильтруем только неправду\n",
    "train_df_balanced_multi = train_df_balanced[train_df_balanced['label'] != 4].copy()\n",
    "valid_df_multi = valid_df[valid_df['label'] != 4].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u3p_W-InG79U"
   },
   "outputs": [],
   "source": [
    "# Ячейка 6: Токенизатор\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ao9UU2kkG-ZE"
   },
   "outputs": [],
   "source": [
    "# Ячейка 7: Dataset для многоклассовой классификации\n",
    "class LiarDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len, label_column='label'):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.label_column = label_column\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        text = row['statement']\n",
    "        labels = row[self.label_column]\n",
    "        inputs = self.tokenizer(text, max_length=self.max_len, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "        item = {key: val.squeeze(0) for key, val in inputs.items()}\n",
    "        item['labels'] = torch.tensor(labels, dtype=torch.long)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "29ACIMGaLpUD"
   },
   "outputs": [],
   "source": [
    "# Ячейка 8.1: Создаем датасеты и загрузчики бинарной модели\n",
    "train_dataset_bin = LiarDataset(train_df_balanced_bin, tokenizer, config.MAX_LEN, label_column='label_bin')\n",
    "valid_dataset_bin = LiarDataset(valid_df_bin, tokenizer, config.MAX_LEN, label_column='label_bin')\n",
    "test_dataset_bin = LiarDataset(test_df_bin, tokenizer, config.MAX_LEN, label_column='label_bin')\n",
    "\n",
    "train_loader_bin = DataLoader(train_dataset_bin, batch_size=config.BATCH_SIZE_TRAIN, shuffle=True, num_workers=config.NUM_WORKERS)\n",
    "valid_loader_bin = DataLoader(valid_dataset_bin, batch_size=config.BATCH_SIZE_VALID, shuffle=False, num_workers=config.NUM_WORKERS)\n",
    "test_loader_bin = DataLoader(test_dataset_bin, batch_size=config.BATCH_SIZE_VALID, shuffle=False, num_workers=config.NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "epoFFPuQHBVF"
   },
   "outputs": [],
   "source": [
    "# Ячейка 8.2: Создаем датасеты и загрузчики вероятностной модели\n",
    "train_dataset_multi = LiarDataset(train_df_balanced_multi, tokenizer, config.MAX_LEN, label_column='label')\n",
    "valid_dataset_multi = LiarDataset(valid_df_multi, tokenizer, config.MAX_LEN, label_column='label')\n",
    "\n",
    "train_loader_multi = DataLoader(train_dataset_multi, batch_size=config.BATCH_SIZE_TRAIN, shuffle=True, num_workers=config.NUM_WORKERS)\n",
    "valid_loader_multi = DataLoader(valid_dataset_multi, batch_size=config.BATCH_SIZE_VALID, shuffle=False, num_workers=config.NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6tq9xD_0HEvD"
   },
   "outputs": [],
   "source": [
    "# Ячейка 9: Модель с выходом для любого количества классов\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, model_name, num_classes, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.config = AutoConfig.from_pretrained(model_name)\n",
    "        self.config.output_hidden_states = False\n",
    "        self.model = AutoModel.from_pretrained(model_name, config=self.config)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.classifier = nn.Linear(self.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state[:,0]  # CLS токен\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ECFNT6DkHJZf"
   },
   "outputs": [],
   "source": [
    "# Ячейка 10: Функции обучения и валидации\n",
    "def train_epoch(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    preds = []\n",
    "    targets = []\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        preds.append(outputs.argmax(dim=1).detach().cpu().numpy())\n",
    "        targets.append(labels.detach().cpu().numpy())\n",
    "\n",
    "    avg_loss = np.mean(losses)\n",
    "    preds = np.concatenate(preds)\n",
    "    targets = np.concatenate(targets)\n",
    "    acc = (preds == targets).mean()\n",
    "    return avg_loss, acc\n",
    "\n",
    "def valid_epoch(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    preds = []\n",
    "    targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Validation\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            preds.append(outputs.argmax(dim=1).detach().cpu().numpy())\n",
    "            targets.append(labels.detach().cpu().numpy())\n",
    "\n",
    "    avg_loss = np.mean(losses)\n",
    "    preds = np.concatenate(preds)\n",
    "    targets = np.concatenate(targets)\n",
    "    acc = (preds == targets).mean()\n",
    "    return avg_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FYdsSs34OY69"
   },
   "outputs": [],
   "source": [
    "# Ячейка 11: EarlyStopping\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, score):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            return False\n",
    "        elif score < self.best_score + self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                return True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "DuFim2LnJaWy"
   },
   "outputs": [],
   "source": [
    "# Ячейка 12.1: Бинарная классификация Правда/Ложь\n",
    "# Ctrl+F8 - выполнить все ячейки выше\n",
    "\n",
    "model_bin = CustomModel(config.MODEL, config.NUM_CLASSES_BIN, 0.3).to(device)\n",
    "#model_bin.load_state_dict(torch.load(os.path.join(output_dir, \"deberta_v3_xsmall_binary_liar.pth\")))\n",
    "\n",
    "optimizer = torch.optim.AdamW(model_bin.parameters(), lr=config.LEARNING_RATE, weight_decay=config.WEIGHT_DECAY_BIN)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2)\n",
    "\n",
    "# Дисбаланс классов\n",
    "class_counts = train_df_balanced_bin['label_bin'].value_counts()\n",
    "pos_weight = class_counts[0] / class_counts[1]\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight).to(device))\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "truth_threshold = 0.3  # начальный порог, будет подбираться\n",
    "early_stopping = EarlyStopping(patience=3, min_delta=1e-4)\n",
    "epochs_no_improve = 0\n",
    "best_macro_f1 = 0\n",
    "best_threshold = truth_threshold\n",
    "\n",
    "looping = True\n",
    "\n",
    "for epoch in range(config.EPOCHS):\n",
    "    model_bin.train()\n",
    "    train_losses = []\n",
    "    train_preds = []\n",
    "    train_targets = []\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    for step, batch in enumerate(tqdm(train_loader_bin, desc=f\"Training Epoch {epoch+1}\")):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device).float().unsqueeze(1)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model_bin(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss = loss / config.GRADIENT_ACCUMULATION_STEPS\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (step + 1) % config.GRADIENT_ACCUMULATION_STEPS == 0 or (step + 1) == len(train_loader_bin):\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        train_losses.append(loss.item() * config.GRADIENT_ACCUMULATION_STEPS)\n",
    "        probs = torch.sigmoid(outputs).detach().cpu()\n",
    "        preds = (probs > truth_threshold).long().numpy()\n",
    "        train_preds.extend(preds)\n",
    "        train_targets.extend(labels.detach().cpu().long().numpy())\n",
    "\n",
    "    train_acc = np.mean(np.array(train_preds) == np.array(train_targets))\n",
    "    train_loss = np.mean(train_losses)\n",
    "\n",
    "    # Валидация\n",
    "    model_bin.eval()\n",
    "    val_losses = []\n",
    "    val_probs = []\n",
    "    val_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(valid_loader_bin, desc=\"Validation\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device).float().unsqueeze(1)\n",
    "\n",
    "            outputs = model_bin(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_losses.append(loss.item())\n",
    "\n",
    "            probs = torch.sigmoid(outputs).cpu().numpy()\n",
    "            val_probs.extend(probs.flatten())\n",
    "            val_targets.extend(labels.cpu().numpy().flatten())\n",
    "\n",
    "    val_loss = np.mean(val_losses)\n",
    "    val_targets_np = np.array(val_targets)\n",
    "\n",
    "    # Подбор оптимального порога по F1 на валидации\n",
    "    from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "    precision, recall, thresholds = precision_recall_curve(val_targets_np, val_probs)\n",
    "    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "    best_idx = np.argmax(f1_scores)\n",
    "    best_threshold_epoch = thresholds[best_idx]\n",
    "    best_f1_epoch = f1_scores[best_idx]\n",
    "\n",
    "    # Классификация с оптимальным порогом\n",
    "    val_preds = (np.array(val_probs) > best_threshold_epoch).astype(int)\n",
    "\n",
    "    val_acc = (val_preds == val_targets_np).mean()\n",
    "    macro_f1 = f1_score(val_targets_np, val_preds, average='macro')\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{config.EPOCHS} | Train loss: {train_loss:.4f} acc: {train_acc:.4f} | Val loss: {val_loss:.4f} acc: {val_acc:.4f} macro F1: {macro_f1:.4f}\")\n",
    "    print(f\"Best threshold this epoch: {best_threshold_epoch:.3f} with macro F1: {best_f1_epoch:.4f}\")\n",
    "    print(\"Classification report (macro):\")\n",
    "    print(classification_report(val_targets_np, val_preds, digits=4, zero_division=0))\n",
    "    print(\"Confusion matrix:\")\n",
    "    print(confusion_matrix(val_targets_np, val_preds))\n",
    "    print(\"\")\n",
    "\n",
    "    scheduler.step(val_acc)\n",
    "\n",
    "    # Ранняя остановка по макро F1\n",
    "    if macro_f1 > best_macro_f1 + 1e-4:\n",
    "        best_macro_f1 = macro_f1\n",
    "        best_threshold = best_threshold_epoch\n",
    "        epochs_no_improve = 0\n",
    "        # Сохраняем лучшую модель\n",
    "        torch.save({\n",
    "            'model_state_dict': model_bin.state_dict(),\n",
    "            'best_threshold': best_threshold\n",
    "        }, os.path.join(output_dir, \"deberta_v3_xsmall_liar_binary_best_with_threshold.pth\"))\n",
    "        print(\"Сохранили лучшую модель и порог.\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= early_stopping.patience:\n",
    "            print(f\"Ранняя остановка на эпохе {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    # user_input = input(\"Хотите продолжить тренировку? (yes/no): \").strip().lower()\n",
    "    # if user_input not in ('yes', 'y'):\n",
    "    #     print(\"Остановка по запросу пользователя.\")\n",
    "    #     break\n",
    "\n",
    "print(f\"Обучение завершено. Лучший macro F1: {best_macro_f1:.4f} при пороге {best_threshold:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4mQF3EvkHMg_"
   },
   "outputs": [],
   "source": [
    "# Ячейка 12.2: Вероятностная классификация степени лжи\n",
    "model = CustomModel(config.MODEL, config.NUM_CLASSES, 0.5).to(device)\n",
    "#model.load_state_dict(torch.load(os.path.join(output_dir, \"deberta_v3_xsmall_probability_liar.pth\")))\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.LEARNING_RATE, weight_decay=config.WEIGHT_DECAY)\n",
    "\n",
    "# Дисбаланс классов\n",
    "class_counts = train_df_balanced['label'].value_counts().sort_index()\n",
    "class_weights = 1.0 / torch.tensor(class_counts.values, dtype=torch.float)\n",
    "class_weights = class_weights / class_weights.sum()\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "looping = True\n",
    "\n",
    "while looping:\n",
    "  for epoch in range(config.EPOCHS):\n",
    "      model.train()\n",
    "      train_losses = []\n",
    "      train_preds = []\n",
    "      train_targets = []\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      for step, batch in enumerate(tqdm(train_loader_multi, desc=f\"Training Epoch {epoch+1}\")):\n",
    "          input_ids = batch['input_ids'].to(device)\n",
    "          attention_mask = batch['attention_mask'].to(device)\n",
    "          labels = batch['labels'].to(device)\n",
    "\n",
    "          with torch.cuda.amp.autocast():\n",
    "              outputs = model(input_ids, attention_mask)\n",
    "              loss = criterion(outputs, labels)\n",
    "              loss = loss / config.GRADIENT_ACCUMULATION_STEPS\n",
    "\n",
    "          scaler.scale(loss).backward()\n",
    "\n",
    "          if (step + 1) % config.GRADIENT_ACCUMULATION_STEPS == 0 or (step + 1) == len(train_loader_multi):\n",
    "              scaler.step(optimizer)\n",
    "              scaler.update()\n",
    "              optimizer.zero_grad()\n",
    "\n",
    "          train_losses.append(loss.item() * config.GRADIENT_ACCUMULATION_STEPS)\n",
    "          preds = outputs.argmax(dim=1).detach().cpu().numpy()\n",
    "          train_preds.extend(preds)\n",
    "          train_targets.extend(labels.detach().cpu().numpy())\n",
    "\n",
    "      train_acc = np.mean(np.array(train_preds) == np.array(train_targets))\n",
    "      train_loss = np.mean(train_losses)\n",
    "\n",
    "      # Валидация (без градиентов)\n",
    "      model.eval()\n",
    "      val_losses = []\n",
    "      val_preds = []\n",
    "      val_targets = []\n",
    "\n",
    "      with torch.no_grad():\n",
    "          for batch in tqdm(valid_loader_multi, desc=\"Validation\"):\n",
    "              input_ids = batch['input_ids'].to(device)\n",
    "              attention_mask = batch['attention_mask'].to(device)\n",
    "              labels = batch['labels'].to(device)\n",
    "\n",
    "              outputs = model(input_ids, attention_mask)\n",
    "              loss = criterion(outputs, labels)\n",
    "\n",
    "              val_losses.append(loss.item())\n",
    "              preds = outputs.argmax(dim=1).detach().cpu().numpy()\n",
    "              val_preds.extend(preds)\n",
    "              val_targets.extend(labels.detach().cpu().numpy())\n",
    "\n",
    "      val_acc = np.mean(np.array(val_preds) == np.array(val_targets))\n",
    "      val_loss = np.mean(val_losses)\n",
    "\n",
    "      print(f\"Epoch {epoch+1}/{config.EPOCHS} | Train loss: {train_loss:.4f} acc: {train_acc:.4f} | Val loss: {val_loss:.4f} acc: {val_acc:.4f}\")\n",
    "      print(\"Classification report (macro):\")\n",
    "      print(classification_report(val_targets, val_preds, digits=4, zero_division=0))\n",
    "      print(\"Macro F1-score:\", f1_score(val_targets, val_preds, average='macro'))\n",
    "      print(\"Confusion matrix:\")\n",
    "      print(confusion_matrix(val_targets, val_preds))\n",
    "      print(\"\")\n",
    "\n",
    "  user_input = input(\"Хотите продолжить тренировку? (yes/no): \").strip().upper()\n",
    "  if \"YES\" in user_input or \"Y\" in user_input:\n",
    "      looping = True\n",
    "  else:\n",
    "      looping = False\n",
    "\n",
    "# Сохранение модели\n",
    "torch.save(model.state_dict(), os.path.join(output_dir, \"deberta_v3_xsmall_probability_liar.pth\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "enninm2tGzzP"
   },
   "outputs": [],
   "source": [
    "# Ячейка 13: Тестирование результата (тестовые данные из датасета)\n",
    "\n",
    "#model_bin.load_state_dict(torch.load(os.path.join(output_dir, \"deberta_v3_xsmall_binary_liar.pth\")))\n",
    "\n",
    "checkpoint = torch.load(os.path.join(output_dir, \"deberta_v3_xsmall_liar_binary_best_with_threshold.pth\"), weights_only=False)\n",
    "model_bin.load_state_dict(checkpoint['model_state_dict'])\n",
    "truth_threshold = checkpoint['best_threshold']\n",
    "print(f\"Загрузили модель с порогом {truth_threshold:.3f}\")\n",
    "\n",
    "\n",
    "\n",
    "model_bin.eval()\n",
    "\n",
    "test_preds = []\n",
    "test_targets = []\n",
    "test_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader_bin, desc=\"Testing\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch.get('labels')\n",
    "        if labels is not None:\n",
    "            labels = labels.to(device).float().unsqueeze(1)\n",
    "\n",
    "        outputs = model_bin(input_ids, attention_mask)  # logits [batch_size, 1]\n",
    "        probs = torch.sigmoid(outputs).cpu().numpy()\n",
    "        preds = (probs > truth_threshold).astype(int).flatten()\n",
    "\n",
    "        test_preds.extend(preds)\n",
    "        test_probs.extend(probs.flatten())\n",
    "\n",
    "        if labels is not None:\n",
    "            test_targets.extend(labels.cpu().numpy().flatten())\n",
    "\n",
    "if len(test_targets) > 0:\n",
    "    test_targets = np.array(test_targets)\n",
    "    test_preds = np.array(test_preds)\n",
    "    print(\"Test Classification Report:\")\n",
    "    print(classification_report(test_targets, test_preds, digits=4, zero_division=0))\n",
    "    print(\"Macro F1-score:\", f1_score(test_targets, test_preds, average='macro'))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(test_targets, test_preds))\n",
    "else:\n",
    "    print(\"Тестовые метки отсутствуют. Выведены только предсказания.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XLddGXKLxxXD"
   },
   "outputs": [],
   "source": [
    "import googletrans\n",
    "print(googletrans.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eHmdBxNGmo0z"
   },
   "outputs": [],
   "source": [
    "# Ячейка 14: Тестирование ручное\n",
    "# Загрузите токенизатор, если ещё не загружен\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.MODEL)\n",
    "\n",
    "model_bin = CustomModel(config.MODEL, config.NUM_CLASSES_BIN, 0.3).to(device)\n",
    "\n",
    "checkpoint = torch.load(os.path.join(output_dir, \"deberta_v3_xsmall_liar_binary_best_with_threshold.pth\"), weights_only=False)\n",
    "model_bin.load_state_dict(checkpoint['model_state_dict'])\n",
    "truth_threshold = checkpoint['best_threshold']\n",
    "print(f\"Загрузили модель с порогом {truth_threshold:.3f}\")\n",
    "\n",
    "model_bin.eval()\n",
    "\n",
    "def predict_text(text, threshold=truth_threshold):\n",
    "    # Токенизация\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model_bin(input_ids, attention_mask)  # [batch_size, 1]\n",
    "        probs = torch.sigmoid(logits).cpu().item()\n",
    "\n",
    "    label = 1 if probs > threshold else 0\n",
    "    label_name = \"Правда\" if label == 1 else \"Ложь\"\n",
    "    return label_name, probs\n",
    "\n",
    "print(\"Введите текст для классификации (пустая строка для выхода):\")\n",
    "\n",
    "from googletrans import Translator\n",
    "\n",
    "translator = Translator()\n",
    "\n",
    "def translate_text(text, dest_lang='en'):\n",
    "    translation = translator.translate(text, dest=dest_lang)\n",
    "    return translation.text\n",
    "\n",
    "while True:\n",
    "    raw_input = input(\">>> \").strip()\n",
    "    if raw_input == \"\":\n",
    "        print(\"Выход.\")\n",
    "        break\n",
    "    try:\n",
    "        translated_text = translate_text(raw_input, dest_lang='en')\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка перевода: {e}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Перевод: {translated_text}\")\n",
    "    pred_label, pred_prob = predict_text(translated_text, truth_threshold)\n",
    "    print(f\"Предсказание: {pred_label} (вероятность: {pred_prob:.4f})\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPgCweVGRQmXLZ5oIbr19pQ",
   "gpuType": "T4",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1bh2dI2F5EdXq8rgLn3BYXgmfAx_TUeyk",
     "timestamp": 1750453271234
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
